{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "id": "ZfD4IQz3KFDX",
    "outputId": "890694c6-0263-4a33-a171-f6c737c5d4bd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setMFX2I01</th>\n",
       "      <th>setMFX1I03</th>\n",
       "      <th>setMFX0I01</th>\n",
       "      <th>setMFA0I03</th>\n",
       "      <th>setMFD0I04</th>\n",
       "      <th>setMFX1dsch2</th>\n",
       "      <th>setMFX2dsch2</th>\n",
       "      <th>setMFX3dsch2</th>\n",
       "      <th>setMQS0L01</th>\n",
       "      <th>setMQJ0L01</th>\n",
       "      <th>...</th>\n",
       "      <th>uf:CSalphay</th>\n",
       "      <th>uf:CSbetax</th>\n",
       "      <th>uf:CSbetay</th>\n",
       "      <th>ms:transmission</th>\n",
       "      <th>dv:transmission</th>\n",
       "      <th>l2:transmission</th>\n",
       "      <th>l3:transmission</th>\n",
       "      <th>mgorank</th>\n",
       "      <th>mgoviolations</th>\n",
       "      <th>mgotasknumber</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.2152388304806e+03</td>\n",
       "      <td>2.4901680478064e+03</td>\n",
       "      <td>1.6974857555250e+03</td>\n",
       "      <td>1.4730307858558e+03</td>\n",
       "      <td>2.5370456895050e+00</td>\n",
       "      <td>2.0170568843222e+03</td>\n",
       "      <td>2.3436046152504e+02</td>\n",
       "      <td>-1.7893199491023e+03</td>\n",
       "      <td>8.5382479658866e+00</td>\n",
       "      <td>-9.7152054753091e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>4.9838674719842e+01</td>\n",
       "      <td>1.0242942599818e+01</td>\n",
       "      <td>3.7074592581300e+01</td>\n",
       "      <td>9.9200000000000e-01</td>\n",
       "      <td>9.9200000000000e-01</td>\n",
       "      <td>9.9200000000000e-01</td>\n",
       "      <td>9.9200000000000e-01</td>\n",
       "      <td>1.0000000000000e+00</td>\n",
       "      <td>8.0000000000000e+00</td>\n",
       "      <td>1.2595000000000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.1853598694463e+03</td>\n",
       "      <td>2.5189579635895e+03</td>\n",
       "      <td>1.6759053178537e+03</td>\n",
       "      <td>1.4409907569959e+03</td>\n",
       "      <td>2.5330375557433e+00</td>\n",
       "      <td>1.9511162651602e+03</td>\n",
       "      <td>-1.9565276638273e+02</td>\n",
       "      <td>-1.8032266938027e+03</td>\n",
       "      <td>-2.0256423977914e+00</td>\n",
       "      <td>-8.6449054843863e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0605702754649e+01</td>\n",
       "      <td>1.7366922294222e+01</td>\n",
       "      <td>1.6258707823379e+01</td>\n",
       "      <td>9.9200000000000e-01</td>\n",
       "      <td>9.9200000000000e-01</td>\n",
       "      <td>9.9200000000000e-01</td>\n",
       "      <td>9.9200000000000e-01</td>\n",
       "      <td>0.0000000000000e+00</td>\n",
       "      <td>8.0000000000000e+00</td>\n",
       "      <td>1.2585000000000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.1990150052370e+03</td>\n",
       "      <td>2.4748446260714e+03</td>\n",
       "      <td>1.7539045965079e+03</td>\n",
       "      <td>1.4566707911939e+03</td>\n",
       "      <td>2.5630570655588e+00</td>\n",
       "      <td>1.9708309493719e+03</td>\n",
       "      <td>-2.8540918167180e+02</td>\n",
       "      <td>-1.8013995069066e+03</td>\n",
       "      <td>1.6501728831718e+01</td>\n",
       "      <td>-8.7556564628551e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0038135661402e+00</td>\n",
       "      <td>3.8306757101638e+01</td>\n",
       "      <td>8.2535544652843e-01</td>\n",
       "      <td>9.9200000000000e-01</td>\n",
       "      <td>9.9200000000000e-01</td>\n",
       "      <td>9.9200000000000e-01</td>\n",
       "      <td>9.9200000000000e-01</td>\n",
       "      <td>0.0000000000000e+00</td>\n",
       "      <td>8.0000000000000e+00</td>\n",
       "      <td>1.2564000000000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.1656876426600e+03</td>\n",
       "      <td>2.4881461439830e+03</td>\n",
       "      <td>1.6385081082975e+03</td>\n",
       "      <td>1.4592150232365e+03</td>\n",
       "      <td>2.5596859035077e+00</td>\n",
       "      <td>1.9506243081822e+03</td>\n",
       "      <td>-4.5639843790867e+02</td>\n",
       "      <td>-1.8339705247665e+03</td>\n",
       "      <td>-5.8970583786553e+00</td>\n",
       "      <td>-8.0995306776341e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>5.8271806981847e+01</td>\n",
       "      <td>8.4445921236786e+00</td>\n",
       "      <td>4.4399540188201e+01</td>\n",
       "      <td>9.9200000000000e-01</td>\n",
       "      <td>9.9200000000000e-01</td>\n",
       "      <td>9.9200000000000e-01</td>\n",
       "      <td>9.9200000000000e-01</td>\n",
       "      <td>0.0000000000000e+00</td>\n",
       "      <td>8.0000000000000e+00</td>\n",
       "      <td>1.2549000000000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.2255784941502e+03</td>\n",
       "      <td>2.4492299687485e+03</td>\n",
       "      <td>1.7253214644756e+03</td>\n",
       "      <td>1.3746813842227e+03</td>\n",
       "      <td>2.5943344969728e+00</td>\n",
       "      <td>1.9040128311501e+03</td>\n",
       "      <td>-5.5205345021747e+02</td>\n",
       "      <td>-1.7887094574080e+03</td>\n",
       "      <td>-1.8681289492147e+00</td>\n",
       "      <td>-6.8980142762574e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.2958751055500e+01</td>\n",
       "      <td>7.8646538481714e+00</td>\n",
       "      <td>4.4392867910089e+01</td>\n",
       "      <td>9.9200000000000e-01</td>\n",
       "      <td>9.9200000000000e-01</td>\n",
       "      <td>9.9200000000000e-01</td>\n",
       "      <td>9.9200000000000e-01</td>\n",
       "      <td>1.0000000000000e+00</td>\n",
       "      <td>8.0000000000000e+00</td>\n",
       "      <td>1.2526000000000e+04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 93 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            setMFX2I01           setMFX1I03           setMFX0I01  \\\n",
       "0  2.2152388304806e+03  2.4901680478064e+03  1.6974857555250e+03   \n",
       "1  2.1853598694463e+03  2.5189579635895e+03  1.6759053178537e+03   \n",
       "2  2.1990150052370e+03  2.4748446260714e+03  1.7539045965079e+03   \n",
       "3  2.1656876426600e+03  2.4881461439830e+03  1.6385081082975e+03   \n",
       "4  2.2255784941502e+03  2.4492299687485e+03  1.7253214644756e+03   \n",
       "\n",
       "            setMFA0I03           setMFD0I04         setMFX1dsch2  \\\n",
       "0  1.4730307858558e+03  2.5370456895050e+00  2.0170568843222e+03   \n",
       "1  1.4409907569959e+03  2.5330375557433e+00  1.9511162651602e+03   \n",
       "2  1.4566707911939e+03  2.5630570655588e+00  1.9708309493719e+03   \n",
       "3  1.4592150232365e+03  2.5596859035077e+00  1.9506243081822e+03   \n",
       "4  1.3746813842227e+03  2.5943344969728e+00  1.9040128311501e+03   \n",
       "\n",
       "           setMFX2dsch2          setMFX3dsch2            setMQS0L01  \\\n",
       "0   2.3436046152504e+02  -1.7893199491023e+03   8.5382479658866e+00   \n",
       "1  -1.9565276638273e+02  -1.8032266938027e+03  -2.0256423977914e+00   \n",
       "2  -2.8540918167180e+02  -1.8013995069066e+03   1.6501728831718e+01   \n",
       "3  -4.5639843790867e+02  -1.8339705247665e+03  -5.8970583786553e+00   \n",
       "4  -5.5205345021747e+02  -1.7887094574080e+03  -1.8681289492147e+00   \n",
       "\n",
       "             setMQJ0L01  ...          uf:CSalphay           uf:CSbetax  \\\n",
       "0  -9.7152054753091e+01  ...  4.9838674719842e+01  1.0242942599818e+01   \n",
       "1  -8.6449054843863e+01  ...  3.0605702754649e+01  1.7366922294222e+01   \n",
       "2  -8.7556564628551e+01  ...  9.0038135661402e+00  3.8306757101638e+01   \n",
       "3  -8.0995306776341e+01  ...  5.8271806981847e+01  8.4445921236786e+00   \n",
       "4  -6.8980142762574e+01  ...  2.2958751055500e+01  7.8646538481714e+00   \n",
       "\n",
       "            uf:CSbetay      ms:transmission      dv:transmission  \\\n",
       "0  3.7074592581300e+01  9.9200000000000e-01  9.9200000000000e-01   \n",
       "1  1.6258707823379e+01  9.9200000000000e-01  9.9200000000000e-01   \n",
       "2  8.2535544652843e-01  9.9200000000000e-01  9.9200000000000e-01   \n",
       "3  4.4399540188201e+01  9.9200000000000e-01  9.9200000000000e-01   \n",
       "4  4.4392867910089e+01  9.9200000000000e-01  9.9200000000000e-01   \n",
       "\n",
       "       l2:transmission      l3:transmission              mgorank  \\\n",
       "0  9.9200000000000e-01  9.9200000000000e-01  1.0000000000000e+00   \n",
       "1  9.9200000000000e-01  9.9200000000000e-01  0.0000000000000e+00   \n",
       "2  9.9200000000000e-01  9.9200000000000e-01  0.0000000000000e+00   \n",
       "3  9.9200000000000e-01  9.9200000000000e-01  0.0000000000000e+00   \n",
       "4  9.9200000000000e-01  9.9200000000000e-01  1.0000000000000e+00   \n",
       "\n",
       "         mgoviolations        mgotasknumber  \n",
       "0  8.0000000000000e+00  1.2595000000000e+04  \n",
       "1  8.0000000000000e+00  1.2585000000000e+04  \n",
       "2  8.0000000000000e+00  1.2564000000000e+04  \n",
       "3  8.0000000000000e+00  1.2549000000000e+04  \n",
       "4  8.0000000000000e+00  1.2526000000000e+04  \n",
       "\n",
       "[5 rows x 93 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#import io\n",
    "#from google.colab import files\n",
    "#uploaded = files.upload()\n",
    "data = pd.read_csv('mgo_latest.csv',  delim_whitespace=True)\n",
    "# Replace 'data.txt' with the path to your .txt file\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        setMFX2I01   setMFX1I03   setMFX0I01   setMFA0I03  setMFD0I04  \\\n",
      "0      2215.238830  2490.168048  1697.485756  1473.030786    2.537046   \n",
      "1      2185.359869  2518.957964  1675.905318  1440.990757    2.533038   \n",
      "2      2199.015005  2474.844626  1753.904597  1456.670791    2.563057   \n",
      "3      2165.687643  2488.146144  1638.508108  1459.215023    2.559686   \n",
      "4      2225.578494  2449.229969  1725.321464  1374.681384    2.594334   \n",
      "...            ...          ...          ...          ...         ...   \n",
      "12715  2194.880000  2486.021754  1638.307218  1427.757120    2.564610   \n",
      "12716  2190.809600  2474.182400  1625.175168  1409.574400    2.543485   \n",
      "12717  2231.710720  2499.636800  1601.835840  1422.000000    2.567682   \n",
      "12718  2211.310720  2477.031360  1625.835840  1426.800000    2.562402   \n",
      "12719  2195.360000  2454.680000  1554.408000  1398.760000    2.560280   \n",
      "\n",
      "       setMFX1dsch2  setMFX2dsch2  setMFX3dsch2  setMQS0L01  setMQJ0L01  ...  \\\n",
      "0       2017.056884    234.360462  -1789.319949    8.538248  -97.152055  ...   \n",
      "1       1951.116265   -195.652766  -1803.226694   -2.025642  -86.449055  ...   \n",
      "2       1970.830949   -285.409182  -1801.399507   16.501729  -87.556565  ...   \n",
      "3       1950.624308   -456.398438  -1833.970525   -5.897058  -80.995307  ...   \n",
      "4       1904.012831   -552.053450  -1788.709457   -1.868129  -68.980143  ...   \n",
      "...             ...           ...           ...         ...         ...  ...   \n",
      "12715   1999.256896   -275.473427  -1795.440000   -9.666392  -85.033552  ...   \n",
      "12716   1941.507200   -400.678720  -1791.998400  -10.434716  -85.788880  ...   \n",
      "12717   1918.555840   -672.676050  -1790.743040   -4.238027  -76.492800  ...   \n",
      "12718   1924.795840   -648.844050  -1798.183040   -7.373464  -77.472000  ...   \n",
      "12719   1926.440000   -455.008000  -1770.720000   -2.938229  -82.930000  ...   \n",
      "\n",
      "       uf:CSalphay  uf:CSbetax  uf:CSbetay  ms:transmission  dv:transmission  \\\n",
      "0        49.838675   10.242943   37.074593            0.992            0.992   \n",
      "1        30.605703   17.366922   16.258708            0.992            0.992   \n",
      "2         9.003814   38.306757    0.825355            0.992            0.992   \n",
      "3        58.271807    8.444592   44.399540            0.992            0.992   \n",
      "4        22.958751    7.864654   44.392868            0.992            0.992   \n",
      "...            ...         ...         ...              ...              ...   \n",
      "12715    25.913170   34.111211    6.759540            0.992            0.992   \n",
      "12716    30.963913   16.379726    8.012462            0.992            0.992   \n",
      "12717     8.969866   23.594605    0.715123            0.992            0.992   \n",
      "12718    15.291065   18.252533    1.882155            0.992            0.992   \n",
      "12719     8.182811   43.817049    0.550880            0.992            0.992   \n",
      "\n",
      "       l2:transmission  l3:transmission  mgorank  mgoviolations  mgotasknumber  \n",
      "0                0.992            0.992      1.0            8.0        12595.0  \n",
      "1                0.992            0.992      0.0            8.0        12585.0  \n",
      "2                0.992            0.992      0.0            8.0        12564.0  \n",
      "3                0.992            0.992      0.0            8.0        12549.0  \n",
      "4                0.992            0.992      1.0            8.0        12526.0  \n",
      "...                ...              ...      ...            ...            ...  \n",
      "12715            0.992            0.992      0.0            8.0         6244.0  \n",
      "12716            0.992            0.992      0.0            8.0         6108.0  \n",
      "12717            0.992            0.992      1.0            8.0         5197.0  \n",
      "12718            0.992            0.992      0.0            8.0         5146.0  \n",
      "12719            0.992            0.992      0.0            8.0         3596.0  \n",
      "\n",
      "[12720 rows x 93 columns]\n"
     ]
    }
   ],
   "source": [
    "data1 = data.apply(lambda x: pd.to_numeric(x, errors='coerce')).dropna()\n",
    "\n",
    "# Reset the index of the dataframe\n",
    "data1 = data1.reset_index(drop=True)\n",
    "\n",
    "# Print the updated dataframe\n",
    "print(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      setMFX2I01   setMFX1I03   setMFX0I01   setMFA0I03  setMFD0I04  \\\n",
      "0    2215.238830  2490.168048  1697.485756  1473.030786    2.537046   \n",
      "1    2185.359869  2518.957964  1675.905318  1440.990757    2.533038   \n",
      "2    2199.015005  2474.844626  1753.904597  1456.670791    2.563057   \n",
      "3    2165.687643  2488.146144  1638.508108  1459.215023    2.559686   \n",
      "4    2225.578494  2449.229969  1725.321464  1374.681384    2.594334   \n",
      "..           ...          ...          ...          ...         ...   \n",
      "115  2194.880000  2486.021754  1638.307218  1427.757120    2.564610   \n",
      "116  2190.809600  2474.182400  1625.175168  1409.574400    2.543485   \n",
      "117  2231.710720  2499.636800  1601.835840  1422.000000    2.567682   \n",
      "118  2211.310720  2477.031360  1625.835840  1426.800000    2.562402   \n",
      "119  2195.360000  2454.680000  1554.408000  1398.760000    2.560280   \n",
      "\n",
      "     setMFX1dsch2  setMFX2dsch2  setMFX3dsch2  setMQS0L01  setMQJ0L01  ...  \\\n",
      "0     2017.056884    234.360462  -1789.319949    8.538248  -97.152055  ...   \n",
      "1     1951.116265   -195.652766  -1803.226694   -2.025642  -86.449055  ...   \n",
      "2     1970.830949   -285.409182  -1801.399507   16.501729  -87.556565  ...   \n",
      "3     1950.624308   -456.398438  -1833.970525   -5.897058  -80.995307  ...   \n",
      "4     1904.012831   -552.053450  -1788.709457   -1.868129  -68.980143  ...   \n",
      "..            ...           ...           ...         ...         ...  ...   \n",
      "115   1999.256896   -275.473427  -1795.440000   -9.666392  -85.033552  ...   \n",
      "116   1941.507200   -400.678720  -1791.998400  -10.434716  -85.788880  ...   \n",
      "117   1918.555840   -672.676050  -1790.743040   -4.238027  -76.492800  ...   \n",
      "118   1924.795840   -648.844050  -1798.183040   -7.373464  -77.472000  ...   \n",
      "119   1926.440000   -455.008000  -1770.720000   -2.938229  -82.930000  ...   \n",
      "\n",
      "     uf:CSalphay  uf:CSbetax  uf:CSbetay  ms:transmission  dv:transmission  \\\n",
      "0      49.838675   10.242943   37.074593            0.992            0.992   \n",
      "1      30.605703   17.366922   16.258708            0.992            0.992   \n",
      "2       9.003814   38.306757    0.825355            0.992            0.992   \n",
      "3      58.271807    8.444592   44.399540            0.992            0.992   \n",
      "4      22.958751    7.864654   44.392868            0.992            0.992   \n",
      "..           ...         ...         ...              ...              ...   \n",
      "115    25.913170   34.111211    6.759540            0.992            0.992   \n",
      "116    30.963913   16.379726    8.012462            0.992            0.992   \n",
      "117     8.969866   23.594605    0.715123            0.992            0.992   \n",
      "118    15.291065   18.252533    1.882155            0.992            0.992   \n",
      "119     8.182811   43.817049    0.550880            0.992            0.992   \n",
      "\n",
      "     l2:transmission  l3:transmission  mgorank  mgoviolations  mgotasknumber  \n",
      "0              0.992            0.992      1.0            8.0        12595.0  \n",
      "1              0.992            0.992      0.0            8.0        12585.0  \n",
      "2              0.992            0.992      0.0            8.0        12564.0  \n",
      "3              0.992            0.992      0.0            8.0        12549.0  \n",
      "4              0.992            0.992      1.0            8.0        12526.0  \n",
      "..               ...              ...      ...            ...            ...  \n",
      "115            0.992            0.992      0.0            8.0         6244.0  \n",
      "116            0.992            0.992      0.0            8.0         6108.0  \n",
      "117            0.992            0.992      1.0            8.0         5197.0  \n",
      "118            0.992            0.992      0.0            8.0         5146.0  \n",
      "119            0.992            0.992      0.0            8.0         3596.0  \n",
      "\n",
      "[120 rows x 93 columns]\n",
      "       setMFX2I01   setMFX1I03   setMFX0I01  setMFA0I03  setMFD0I04  \\\n",
      "120    2134.00000  2449.000000  1690.000000  1423.00000    2.552000   \n",
      "121    2207.00000  2462.000000  1590.000000  1379.00000    2.572000   \n",
      "122    2227.00000  2519.000000  1623.000000  1438.00000    2.524000   \n",
      "123    2208.00000  2513.000000  1679.000000  1422.00000    2.559000   \n",
      "124    2192.00000  2442.000000  1628.000000  1416.00000    2.562000   \n",
      "...           ...          ...          ...         ...         ...   \n",
      "12715  2194.88000  2486.021754  1638.307218  1427.75712    2.564610   \n",
      "12716  2190.80960  2474.182400  1625.175168  1409.57440    2.543485   \n",
      "12717  2231.71072  2499.636800  1601.835840  1422.00000    2.567682   \n",
      "12718  2211.31072  2477.031360  1625.835840  1426.80000    2.562402   \n",
      "12719  2195.36000  2454.680000  1554.408000  1398.76000    2.560280   \n",
      "\n",
      "       setMFX1dsch2  setMFX2dsch2  setMFX3dsch2  setMQS0L01  setMQJ0L01  ...  \\\n",
      "120     1890.000000   -338.200000   -1784.00000   22.440000  -68.540000  ...   \n",
      "121     1962.000000   -601.200000   -1809.00000   -2.728000  -82.930000  ...   \n",
      "122     1991.000000   -345.700000   -1799.00000  -15.870000  -79.810000  ...   \n",
      "123     1921.000000   -348.000000   -1785.00000    9.219000  -91.740000  ...   \n",
      "124     1965.000000   -514.900000   -1802.00000   18.360000  -61.260000  ...   \n",
      "...             ...           ...           ...         ...         ...  ...   \n",
      "12715   1999.256896   -275.473427   -1795.44000   -9.666392  -85.033552  ...   \n",
      "12716   1941.507200   -400.678720   -1791.99840  -10.434716  -85.788880  ...   \n",
      "12717   1918.555840   -672.676050   -1790.74304   -4.238027  -76.492800  ...   \n",
      "12718   1924.795840   -648.844050   -1798.18304   -7.373464  -77.472000  ...   \n",
      "12719   1926.440000   -455.008000   -1770.72000   -2.938229  -82.930000  ...   \n",
      "\n",
      "       uf:CSalphay  uf:CSbetax  uf:CSbetay  ms:transmission  dv:transmission  \\\n",
      "120       1.529411    3.513340    0.171773            0.992            0.992   \n",
      "121      36.931894   17.417113   14.993918            0.992            0.992   \n",
      "122      44.931038    4.825082   24.436395            0.992            0.992   \n",
      "123      19.457429   30.342688    4.831741            0.992            0.992   \n",
      "124      -6.804103   15.685770    0.314425            0.992            0.992   \n",
      "...            ...         ...         ...              ...              ...   \n",
      "12715    25.913170   34.111211    6.759540            0.992            0.992   \n",
      "12716    30.963913   16.379726    8.012462            0.992            0.992   \n",
      "12717     8.969866   23.594605    0.715123            0.992            0.992   \n",
      "12718    15.291065   18.252533    1.882155            0.992            0.992   \n",
      "12719     8.182811   43.817049    0.550880            0.992            0.992   \n",
      "\n",
      "       l2:transmission  l3:transmission  mgorank  mgoviolations  mgotasknumber  \n",
      "120              0.992            0.992      0.0            8.0          120.0  \n",
      "121              0.992            0.992      6.0            9.0          118.0  \n",
      "122              0.992            0.992      5.0            9.0          119.0  \n",
      "123              0.992            0.992      7.0            9.0          115.0  \n",
      "124              0.992            0.992      6.0            9.0          117.0  \n",
      "...                ...              ...      ...            ...            ...  \n",
      "12715            0.992            0.992      0.0            8.0         6244.0  \n",
      "12716            0.992            0.992      0.0            8.0         6108.0  \n",
      "12717            0.992            0.992      1.0            8.0         5197.0  \n",
      "12718            0.992            0.992      0.0            8.0         5146.0  \n",
      "12719            0.992            0.992      0.0            8.0         3596.0  \n",
      "\n",
      "[12600 rows x 93 columns]\n"
     ]
    }
   ],
   "source": [
    "data2 = data1.iloc[:120]\n",
    "data3 = data1.iloc[120:]\n",
    "print(data2)\n",
    "print(data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 2.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t7/yy62typn4y1cknks8l305k2h0000gn/T/ipykernel_15021/1778657708.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data2['mgorank'] = pd.to_numeric(data2['mgorank'], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data from your file (replace 'mgo_latest.csv' with your actual file name)\n",
    "#df = pd.read_csv('mgo_latest.csv', delim_whitespace=True)\n",
    "\n",
    "# Convert 'magorank' column to numeric, skipping rows with errors\n",
    "data2['mgorank'] = pd.to_numeric(data2['mgorank'], errors='coerce')\n",
    "\n",
    "# Print the unique values in the 'magorank' column\n",
    "print(data2['mgorank'].unique())\n",
    "\n",
    "# Select rows where 'magorank' is exactly 0.0000000000000e+00\n",
    "selected_data = data2[data2['mgorank'] == 0.0000000000000e+00]\n",
    "\n",
    "# Save the selected data to a text file (replace 'selected_data.txt' with your desired file name)\n",
    "selected_data.to_csv('selected_data.txt', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the selected data from the text file\n",
    "selected_data = pd.read_csv('selected_data.txt', sep='\\t')\n",
    "\n",
    "# Choose two columns for the plot (replace 'column1' and 'column2' with actual column names)\n",
    "column1 = 'uf:nemixrms'  # Replace with the actual column name\n",
    "column2 = 'uf:nemiyrms'# Replace with the actual column name\n",
    "column3 = 'uf:stdt'\n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(selected_data[column1], selected_data[column3])\n",
    "plt.scatter(selected_data[column2], selected_data[column3])\n",
    "plt.xlabel(column1)\n",
    "plt.ylabel(column2)\n",
    "#plt.title('Scatter Plot of {} vs {}'.format(column1, column2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after removing rows and resetting index:\n",
      "       setMFX2I01   setMFX1I03   setMFX0I01  setMFA0I03  setMFD0I04  \\\n",
      "0      2134.00000  2449.000000  1690.000000  1423.00000    2.552000   \n",
      "1      2207.00000  2462.000000  1590.000000  1379.00000    2.572000   \n",
      "2      2227.00000  2519.000000  1623.000000  1438.00000    2.524000   \n",
      "3      2208.00000  2513.000000  1679.000000  1422.00000    2.559000   \n",
      "4      2192.00000  2442.000000  1628.000000  1416.00000    2.562000   \n",
      "...           ...          ...          ...         ...         ...   \n",
      "12595  2194.88000  2486.021754  1638.307218  1427.75712    2.564610   \n",
      "12596  2190.80960  2474.182400  1625.175168  1409.57440    2.543485   \n",
      "12597  2231.71072  2499.636800  1601.835840  1422.00000    2.567682   \n",
      "12598  2211.31072  2477.031360  1625.835840  1426.80000    2.562402   \n",
      "12599  2195.36000  2454.680000  1554.408000  1398.76000    2.560280   \n",
      "\n",
      "       setMFX1dsch2  setMFX2dsch2  setMFX3dsch2  setMQS0L01  setMQJ0L01  ...  \\\n",
      "0       1890.000000   -338.200000   -1784.00000   22.440000  -68.540000  ...   \n",
      "1       1962.000000   -601.200000   -1809.00000   -2.728000  -82.930000  ...   \n",
      "2       1991.000000   -345.700000   -1799.00000  -15.870000  -79.810000  ...   \n",
      "3       1921.000000   -348.000000   -1785.00000    9.219000  -91.740000  ...   \n",
      "4       1965.000000   -514.900000   -1802.00000   18.360000  -61.260000  ...   \n",
      "...             ...           ...           ...         ...         ...  ...   \n",
      "12595   1999.256896   -275.473427   -1795.44000   -9.666392  -85.033552  ...   \n",
      "12596   1941.507200   -400.678720   -1791.99840  -10.434716  -85.788880  ...   \n",
      "12597   1918.555840   -672.676050   -1790.74304   -4.238027  -76.492800  ...   \n",
      "12598   1924.795840   -648.844050   -1798.18304   -7.373464  -77.472000  ...   \n",
      "12599   1926.440000   -455.008000   -1770.72000   -2.938229  -82.930000  ...   \n",
      "\n",
      "       uf:CSalphay  uf:CSbetax  uf:CSbetay  ms:transmission  dv:transmission  \\\n",
      "0         1.529411    3.513340    0.171773            0.992            0.992   \n",
      "1        36.931894   17.417113   14.993918            0.992            0.992   \n",
      "2        44.931038    4.825082   24.436395            0.992            0.992   \n",
      "3        19.457429   30.342688    4.831741            0.992            0.992   \n",
      "4        -6.804103   15.685770    0.314425            0.992            0.992   \n",
      "...            ...         ...         ...              ...              ...   \n",
      "12595    25.913170   34.111211    6.759540            0.992            0.992   \n",
      "12596    30.963913   16.379726    8.012462            0.992            0.992   \n",
      "12597     8.969866   23.594605    0.715123            0.992            0.992   \n",
      "12598    15.291065   18.252533    1.882155            0.992            0.992   \n",
      "12599     8.182811   43.817049    0.550880            0.992            0.992   \n",
      "\n",
      "       l2:transmission  l3:transmission  mgorank  mgoviolations  mgotasknumber  \n",
      "0                0.992            0.992      0.0            8.0          120.0  \n",
      "1                0.992            0.992      6.0            9.0          118.0  \n",
      "2                0.992            0.992      5.0            9.0          119.0  \n",
      "3                0.992            0.992      7.0            9.0          115.0  \n",
      "4                0.992            0.992      6.0            9.0          117.0  \n",
      "...                ...              ...      ...            ...            ...  \n",
      "12595            0.992            0.992      0.0            8.0         6244.0  \n",
      "12596            0.992            0.992      0.0            8.0         6108.0  \n",
      "12597            0.992            0.992      1.0            8.0         5197.0  \n",
      "12598            0.992            0.992      0.0            8.0         5146.0  \n",
      "12599            0.992            0.992      0.0            8.0         3596.0  \n",
      "\n",
      "[12600 rows x 93 columns]\n"
     ]
    }
   ],
   "source": [
    "# Reset the index after removing the rows\n",
    "data3.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the DataFrame after removing rows and resetting the index\n",
    "print(\"\\nDataFrame after removing rows and resetting index:\")\n",
    "print(data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7BguaRFsituA",
    "outputId": "8e351298-e7e5-418b-9b6c-ca04cc44501e"
   },
   "outputs": [],
   "source": [
    "# Perform the arithmetic operation (e.g., addition) on the column values\n",
    "# Replace 'operation' with your desired arithmetic operation (e.g., '+', '*', '-', '/')\n",
    "#df['uf:beam-loss'] = 1.0 - df['uf:transmission']\n",
    "\n",
    "# Drop the old column if needed\n",
    "#df.drop(columns=['uf:transmission'], inplace=True)\n",
    "\n",
    "# Display the DataFrame with the new column\n",
    "#print(\"\\nDataFrame with renamed column:\")\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aQUQz1a-i3Cn",
    "outputId": "17ca4762-f685-497d-c3c6-2aabec16bbf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   setMFX2I01  setMFX1I03  setMFX0I01  setMFA0I03  setMFD0I04  setMFX1dsch2  \\\n",
      "0      2134.0      2449.0      1690.0      1423.0       2.552        1890.0   \n",
      "1      2207.0      2462.0      1590.0      1379.0       2.572        1962.0   \n",
      "2      2227.0      2519.0      1623.0      1438.0       2.524        1991.0   \n",
      "3      2208.0      2513.0      1679.0      1422.0       2.559        1921.0   \n",
      "4      2192.0      2442.0      1628.0      1416.0       2.562        1965.0   \n",
      "\n",
      "   setMFX2dsch2  setMFX3dsch2  setMQS0L01  setMQJ0L01  ...  buncherset  \\\n",
      "0        -338.2       -1784.0      22.440      -68.54  ...    0.000724   \n",
      "1        -601.2       -1809.0      -2.728      -82.93  ...    0.000663   \n",
      "2        -345.7       -1799.0     -15.870      -79.81  ...    0.000755   \n",
      "3        -348.0       -1785.0       9.219      -91.74  ...    0.000643   \n",
      "4        -514.9       -1802.0      18.360      -61.26  ...    0.000654   \n",
      "\n",
      "   booster07set  booster08set   uf:nemixrms   uf:nemiyrms       uf:stdt  \\\n",
      "0     2400000.0     9857000.0  2.425477e-07  2.339496e-07  4.858130e-13   \n",
      "1     2447000.0     9968000.0  2.090040e-07  2.204499e-07  5.066697e-13   \n",
      "2     2453000.0     9873000.0  2.250846e-07  2.400364e-07  5.003096e-13   \n",
      "3     2451000.0     9866000.0  2.133653e-07  2.362039e-07  5.078994e-13   \n",
      "4     2416000.0     9899000.0  2.249011e-07  2.325387e-07  5.062056e-13   \n",
      "\n",
      "   uf:transmission   uf:stdx   uf:stdy   uf:stdEk_eV  \n",
      "0            0.992  0.000243  0.000053  72945.449253  \n",
      "1            0.992  0.000500  0.000476  78095.413464  \n",
      "2            0.992  0.000274  0.000636  72970.520305  \n",
      "3            0.992  0.000668  0.000281  74176.396571  \n",
      "4            0.992  0.000493  0.000071  70795.968723  \n",
      "\n",
      "[5 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'original_dataset' is your original DataFrame with many columns\n",
    "# and 'columns_to_keep' is a list of 40 column names you want to select\n",
    "\n",
    "# Example list of 40 column names you want to keep\n",
    "features = ['setMFX2I01', 'setMFX1I03', 'setMFX0I01', 'setMFA0I03', 'setMFD0I04',\n",
    "       'setMFX1dsch2', 'setMFX2dsch2', 'setMFX3dsch2', 'setMQS0L01',\n",
    "       'setMQJ0L01', 'setMQS0L01A', 'setMQJ0L02', 'setMQS0L02', 'setMQJ0L02A',\n",
    "       'setMQS0L02B', 'setMQJ0L03A', 'setMQS0L03', 'setMQJ0L03', 'setMQS0L04',\n",
    "       'setMQJ0L04', 'PREBUNCHERphase', 'BUNCHERphase', 'booster07phase',\n",
    "       'booster08phase', 'PREBUNCHERset', 'buncherset', 'booster07set',\n",
    "       'booster08set', 'uf:nemixrms', 'uf:nemiyrms', 'uf:stdt',\n",
    "        'uf:transmission', 'uf:stdx','uf:stdy', 'uf:stdEk_eV']\n",
    "\n",
    "# Create a new DataFrame with only the 40 selected columns\n",
    "new_dataset = data3[features]\n",
    "\n",
    "# Print the first few rows of the new dataset\n",
    "print(new_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1d2tYqIli_pq",
    "outputId": "0ebc4842-27cd-4d21-d3b6-250004e782b9"
   },
   "outputs": [],
   "source": [
    "# Assuming you have a DataFrame called 'df'\n",
    "\n",
    "# Display the original DataFrame 'df'\n",
    "#print(\"Original DataFrame:\")\n",
    "#print(new_dataset)\n",
    "\n",
    "# Remove rows from index 1 to 120\n",
    "#df1 = new_dataset.iloc[120:]\n",
    "#df2 = new_dataset.iloc[:121]\n",
    "# Remove rows from index 1 to 120\n",
    "#df1 = new_dataset.iloc[120:]\n",
    "\n",
    "# Reset the index after removing the rows\n",
    "#df1.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the DataFrame after removing rows and resetting the index\n",
    "#print(\"\\nDataFrame after removing rows and resetting index:\")\n",
    "#print(df1)\n",
    "#print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "O_EXAlSBpq5g"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GaussianNoise, Input, Flatten, Conv1D\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "BQ3buBo2paRd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df1 is your original DataFrame\n",
    "df2 = new_dataset.sample(n=10000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 679
    },
    "id": "Tn9p4WqLpdVu",
    "outputId": "5f5fdb5b-43ac-4d18-b4b9-d20500b56620"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setMFX2I01</th>\n",
       "      <th>setMFX1I03</th>\n",
       "      <th>setMFX0I01</th>\n",
       "      <th>setMFA0I03</th>\n",
       "      <th>setMFD0I04</th>\n",
       "      <th>setMFX1dsch2</th>\n",
       "      <th>setMFX2dsch2</th>\n",
       "      <th>setMFX3dsch2</th>\n",
       "      <th>setMQS0L01</th>\n",
       "      <th>setMQJ0L01</th>\n",
       "      <th>...</th>\n",
       "      <th>buncherset</th>\n",
       "      <th>booster07set</th>\n",
       "      <th>booster08set</th>\n",
       "      <th>uf:nemixrms</th>\n",
       "      <th>uf:nemiyrms</th>\n",
       "      <th>uf:stdt</th>\n",
       "      <th>uf:transmission</th>\n",
       "      <th>uf:stdx</th>\n",
       "      <th>uf:stdy</th>\n",
       "      <th>uf:stdEk_eV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11385</th>\n",
       "      <td>2170.531200</td>\n",
       "      <td>2445.915200</td>\n",
       "      <td>1646.487360</td>\n",
       "      <td>1436.668480</td>\n",
       "      <td>2.565446</td>\n",
       "      <td>1961.451520</td>\n",
       "      <td>-445.716288</td>\n",
       "      <td>-1777.915520</td>\n",
       "      <td>-21.258565</td>\n",
       "      <td>-75.897520</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>2.444347e+06</td>\n",
       "      <td>9.852720e+06</td>\n",
       "      <td>2.187895e-07</td>\n",
       "      <td>2.475971e-07</td>\n",
       "      <td>4.499759e-13</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>68707.884734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6866</th>\n",
       "      <td>2222.469568</td>\n",
       "      <td>2456.765376</td>\n",
       "      <td>1701.465600</td>\n",
       "      <td>1404.533184</td>\n",
       "      <td>2.583910</td>\n",
       "      <td>1922.379520</td>\n",
       "      <td>-469.999571</td>\n",
       "      <td>-1799.278400</td>\n",
       "      <td>-9.689123</td>\n",
       "      <td>-67.785261</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000846</td>\n",
       "      <td>2.437280e+06</td>\n",
       "      <td>9.896665e+06</td>\n",
       "      <td>2.113783e-07</td>\n",
       "      <td>2.375488e-07</td>\n",
       "      <td>4.667473e-13</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>68254.055086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1323</th>\n",
       "      <td>2223.000000</td>\n",
       "      <td>2430.000000</td>\n",
       "      <td>1693.000000</td>\n",
       "      <td>1352.000000</td>\n",
       "      <td>2.570000</td>\n",
       "      <td>1978.000000</td>\n",
       "      <td>-226.700000</td>\n",
       "      <td>-1794.000000</td>\n",
       "      <td>22.170000</td>\n",
       "      <td>-74.360000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000728</td>\n",
       "      <td>2.361000e+06</td>\n",
       "      <td>9.979000e+06</td>\n",
       "      <td>2.282923e-07</td>\n",
       "      <td>2.453533e-07</td>\n",
       "      <td>4.964092e-13</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>72520.840162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2252</th>\n",
       "      <td>2232.000000</td>\n",
       "      <td>2486.000000</td>\n",
       "      <td>1583.000000</td>\n",
       "      <td>1431.000000</td>\n",
       "      <td>2.529000</td>\n",
       "      <td>1976.000000</td>\n",
       "      <td>-540.700000</td>\n",
       "      <td>-1805.000000</td>\n",
       "      <td>-35.280000</td>\n",
       "      <td>-64.180000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>2.487000e+06</td>\n",
       "      <td>9.815000e+06</td>\n",
       "      <td>2.323238e-07</td>\n",
       "      <td>2.479794e-07</td>\n",
       "      <td>4.930682e-13</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>70402.692079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11277</th>\n",
       "      <td>2195.360000</td>\n",
       "      <td>2454.680000</td>\n",
       "      <td>1554.408000</td>\n",
       "      <td>1398.760000</td>\n",
       "      <td>2.560280</td>\n",
       "      <td>1926.440000</td>\n",
       "      <td>-455.008000</td>\n",
       "      <td>-1770.720000</td>\n",
       "      <td>-2.938229</td>\n",
       "      <td>-82.930000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000684</td>\n",
       "      <td>2.436000e+06</td>\n",
       "      <td>9.910880e+06</td>\n",
       "      <td>2.135899e-07</td>\n",
       "      <td>2.268660e-07</td>\n",
       "      <td>4.823627e-13</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>72785.530875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2066</th>\n",
       "      <td>2204.800000</td>\n",
       "      <td>2475.312000</td>\n",
       "      <td>1698.000000</td>\n",
       "      <td>1397.280000</td>\n",
       "      <td>2.547888</td>\n",
       "      <td>2001.856000</td>\n",
       "      <td>-396.731200</td>\n",
       "      <td>-1813.400000</td>\n",
       "      <td>-34.244747</td>\n",
       "      <td>-87.060880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000819</td>\n",
       "      <td>2.416400e+06</td>\n",
       "      <td>9.873000e+06</td>\n",
       "      <td>2.370688e-07</td>\n",
       "      <td>2.414322e-07</td>\n",
       "      <td>4.951926e-13</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>65571.708537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1653</th>\n",
       "      <td>2197.000000</td>\n",
       "      <td>2508.000000</td>\n",
       "      <td>1677.000000</td>\n",
       "      <td>1368.000000</td>\n",
       "      <td>2.566000</td>\n",
       "      <td>1841.000000</td>\n",
       "      <td>-535.200000</td>\n",
       "      <td>-1769.000000</td>\n",
       "      <td>18.160000</td>\n",
       "      <td>-82.820000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>2.437000e+06</td>\n",
       "      <td>9.781000e+06</td>\n",
       "      <td>2.133406e-07</td>\n",
       "      <td>2.418159e-07</td>\n",
       "      <td>5.019553e-13</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>78234.419329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8894</th>\n",
       "      <td>2171.268365</td>\n",
       "      <td>2457.917696</td>\n",
       "      <td>1671.379085</td>\n",
       "      <td>1439.977715</td>\n",
       "      <td>2.555614</td>\n",
       "      <td>1947.687675</td>\n",
       "      <td>-430.666281</td>\n",
       "      <td>-1800.521390</td>\n",
       "      <td>-20.311403</td>\n",
       "      <td>-81.025200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>2.459233e+06</td>\n",
       "      <td>9.983974e+06</td>\n",
       "      <td>2.242423e-07</td>\n",
       "      <td>2.489760e-07</td>\n",
       "      <td>4.531189e-13</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.000773</td>\n",
       "      <td>71615.720053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8706</th>\n",
       "      <td>2250.481458</td>\n",
       "      <td>2530.263210</td>\n",
       "      <td>1652.021782</td>\n",
       "      <td>1404.087396</td>\n",
       "      <td>2.561335</td>\n",
       "      <td>1903.158381</td>\n",
       "      <td>-568.934980</td>\n",
       "      <td>-1773.218818</td>\n",
       "      <td>19.718817</td>\n",
       "      <td>-77.905576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000808</td>\n",
       "      <td>2.365062e+06</td>\n",
       "      <td>9.956685e+06</td>\n",
       "      <td>2.132784e-07</td>\n",
       "      <td>2.484526e-07</td>\n",
       "      <td>4.736447e-13</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>67812.715326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1376</th>\n",
       "      <td>2196.000000</td>\n",
       "      <td>2435.600000</td>\n",
       "      <td>1587.600000</td>\n",
       "      <td>1451.800000</td>\n",
       "      <td>2.584000</td>\n",
       "      <td>2061.600000</td>\n",
       "      <td>-250.640000</td>\n",
       "      <td>-1781.600000</td>\n",
       "      <td>32.494600</td>\n",
       "      <td>-77.472000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>2.423000e+06</td>\n",
       "      <td>9.956800e+06</td>\n",
       "      <td>2.255690e-07</td>\n",
       "      <td>2.497556e-07</td>\n",
       "      <td>4.746275e-13</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.000573</td>\n",
       "      <td>77348.469174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        setMFX2I01   setMFX1I03   setMFX0I01   setMFA0I03  setMFD0I04  \\\n",
       "11385  2170.531200  2445.915200  1646.487360  1436.668480    2.565446   \n",
       "6866   2222.469568  2456.765376  1701.465600  1404.533184    2.583910   \n",
       "1323   2223.000000  2430.000000  1693.000000  1352.000000    2.570000   \n",
       "2252   2232.000000  2486.000000  1583.000000  1431.000000    2.529000   \n",
       "11277  2195.360000  2454.680000  1554.408000  1398.760000    2.560280   \n",
       "...            ...          ...          ...          ...         ...   \n",
       "2066   2204.800000  2475.312000  1698.000000  1397.280000    2.547888   \n",
       "1653   2197.000000  2508.000000  1677.000000  1368.000000    2.566000   \n",
       "8894   2171.268365  2457.917696  1671.379085  1439.977715    2.555614   \n",
       "8706   2250.481458  2530.263210  1652.021782  1404.087396    2.561335   \n",
       "1376   2196.000000  2435.600000  1587.600000  1451.800000    2.584000   \n",
       "\n",
       "       setMFX1dsch2  setMFX2dsch2  setMFX3dsch2  setMQS0L01  setMQJ0L01  ...  \\\n",
       "11385   1961.451520   -445.716288  -1777.915520  -21.258565  -75.897520  ...   \n",
       "6866    1922.379520   -469.999571  -1799.278400   -9.689123  -67.785261  ...   \n",
       "1323    1978.000000   -226.700000  -1794.000000   22.170000  -74.360000  ...   \n",
       "2252    1976.000000   -540.700000  -1805.000000  -35.280000  -64.180000  ...   \n",
       "11277   1926.440000   -455.008000  -1770.720000   -2.938229  -82.930000  ...   \n",
       "...             ...           ...           ...         ...         ...  ...   \n",
       "2066    2001.856000   -396.731200  -1813.400000  -34.244747  -87.060880  ...   \n",
       "1653    1841.000000   -535.200000  -1769.000000   18.160000  -82.820000  ...   \n",
       "8894    1947.687675   -430.666281  -1800.521390  -20.311403  -81.025200  ...   \n",
       "8706    1903.158381   -568.934980  -1773.218818   19.718817  -77.905576  ...   \n",
       "1376    2061.600000   -250.640000  -1781.600000   32.494600  -77.472000  ...   \n",
       "\n",
       "       buncherset  booster07set  booster08set   uf:nemixrms   uf:nemiyrms  \\\n",
       "11385    0.000871  2.444347e+06  9.852720e+06  2.187895e-07  2.475971e-07   \n",
       "6866     0.000846  2.437280e+06  9.896665e+06  2.113783e-07  2.375488e-07   \n",
       "1323     0.000728  2.361000e+06  9.979000e+06  2.282923e-07  2.453533e-07   \n",
       "2252     0.000697  2.487000e+06  9.815000e+06  2.323238e-07  2.479794e-07   \n",
       "11277    0.000684  2.436000e+06  9.910880e+06  2.135899e-07  2.268660e-07   \n",
       "...           ...           ...           ...           ...           ...   \n",
       "2066     0.000819  2.416400e+06  9.873000e+06  2.370688e-07  2.414322e-07   \n",
       "1653     0.000713  2.437000e+06  9.781000e+06  2.133406e-07  2.418159e-07   \n",
       "8894     0.000837  2.459233e+06  9.983974e+06  2.242423e-07  2.489760e-07   \n",
       "8706     0.000808  2.365062e+06  9.956685e+06  2.132784e-07  2.484526e-07   \n",
       "1376     0.000812  2.423000e+06  9.956800e+06  2.255690e-07  2.497556e-07   \n",
       "\n",
       "            uf:stdt  uf:transmission   uf:stdx   uf:stdy   uf:stdEk_eV  \n",
       "11385  4.499759e-13            0.992  0.000615  0.000704  68707.884734  \n",
       "6866   4.667473e-13            0.992  0.000277  0.000817  68254.055086  \n",
       "1323   4.964092e-13            0.992  0.000513  0.000109  72520.840162  \n",
       "2252   4.930682e-13            0.992  0.000404  0.000734  70402.692079  \n",
       "11277  4.823627e-13            0.992  0.000802  0.000093  72785.530875  \n",
       "...             ...              ...       ...       ...           ...  \n",
       "2066   4.951926e-13            0.992  0.000510  0.000462  65571.708537  \n",
       "1653   5.019553e-13            0.992  0.000401  0.000202  78234.419329  \n",
       "8894   4.531189e-13            0.992  0.000521  0.000773  71615.720053  \n",
       "8706   4.736447e-13            0.992  0.000369  0.000294  67812.715326  \n",
       "1376   4.746275e-13            0.992  0.000441  0.000573  77348.469174  \n",
       "\n",
       "[10000 rows x 35 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZAYmXd6VjJSl",
    "outputId": "7590fb8d-44b5-44de-d67f-9a861f72f756"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.17053120e+03 2.44591520e+03 1.64648736e+03 ... 8.70872032e-04\n",
      "  2.44434688e+06 9.85272000e+06]\n",
      " [2.22246957e+03 2.45676538e+03 1.70146560e+03 ... 8.45636934e-04\n",
      "  2.43728000e+06 9.89666528e+06]\n",
      " [2.22300000e+03 2.43000000e+03 1.69300000e+03 ... 7.28400000e-04\n",
      "  2.36100000e+06 9.97900000e+06]\n",
      " ...\n",
      " [2.17126836e+03 2.45791770e+03 1.67137908e+03 ... 8.36535850e-04\n",
      "  2.45923313e+06 9.98397357e+06]\n",
      " [2.25048146e+03 2.53026321e+03 1.65202178e+03 ... 8.07592655e-04\n",
      "  2.36506169e+06 9.95668493e+06]\n",
      " [2.19600000e+03 2.43560000e+03 1.58760000e+03 ... 8.11960000e-04\n",
      "  2.42300000e+06 9.95680000e+06]] [[2.18789485e-07 2.47597147e-07 4.49975941e-13 ... 6.14563195e-04\n",
      "  7.03641676e-04 6.87078847e+04]\n",
      " [2.11378258e-07 2.37548805e-07 4.66747346e-13 ... 2.76965707e-04\n",
      "  8.16591304e-04 6.82540551e+04]\n",
      " [2.28292302e-07 2.45353301e-07 4.96409168e-13 ... 5.12814702e-04\n",
      "  1.08663454e-04 7.25208402e+04]\n",
      " ...\n",
      " [2.24242251e-07 2.48975963e-07 4.53118946e-13 ... 5.20502156e-04\n",
      "  7.72759441e-04 7.16157201e+04]\n",
      " [2.13278387e-07 2.48452560e-07 4.73644676e-13 ... 3.68903207e-04\n",
      "  2.93795934e-04 6.78127153e+04]\n",
      " [2.25568999e-07 2.49755556e-07 4.74627514e-13 ... 4.41282049e-04\n",
      "  5.72573307e-04 7.73484692e+04]]\n"
     ]
    }
   ],
   "source": [
    "X = df2.iloc[:, :28].values  # Extract the first 28 columns as input data (variables)\n",
    "y = df2.iloc[:, 28:].values  # Extract the last 7 columns as output data (objectives)\n",
    "\n",
    "\n",
    "print(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "f7cuRb2En2rx"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler_X = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler_y = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Fit and transform X and y separately\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y)\n",
    "\n",
    "# To get back to the original data from the scaled data\n",
    "#x_original = scaler_x.inverse_transform(x_scaled)\n",
    "#y_original = scaler_y.inverse_transform(y_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tk0Q96eAqV8Y",
    "outputId": "7831ce0c-954d-4ceb-e00c-ad09f8818a34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.36396209 -0.2709422   0.08516773 ...  0.34747539  0.2385237\n",
      "   0.16282541]\n",
      " [ 0.06767138 -0.15203022  0.34507772 ...  0.22545499  0.16663276\n",
      "   0.41322078]\n",
      " [ 0.07207953 -0.44536406  0.30505654 ... -0.34142602 -0.6093591\n",
      "   0.88235497]\n",
      " ...\n",
      " [-0.35783589 -0.13940142  0.20284352 ...  0.18144811  0.38996059\n",
      "   0.91069381]\n",
      " [ 0.30046401  0.65346575  0.11133175 ...  0.04149778 -0.56803979\n",
      "   0.75520617]\n",
      " [-0.15230379 -0.38399113 -0.19322265 ...  0.0626154   0.02136317\n",
      "   0.75586181]] [[-0.68979465 -0.7699023  -0.75939189 ... -0.49669717 -0.32455982\n",
      "  -0.23860098]\n",
      " [-0.77968519 -0.83620936 -0.51943641 ... -0.86549452 -0.21453619\n",
      "  -0.26868565]\n",
      " [-0.57453529 -0.784709   -0.09505234 ... -0.60784899 -0.90412495\n",
      "   0.01416242]\n",
      " ...\n",
      " [-0.62365823 -0.76080376 -0.71442361 ... -0.59945108 -0.25723258\n",
      "  -0.0458386 ]\n",
      " [-0.7566386  -0.7642576  -0.42075343 ... -0.7650604  -0.72378838\n",
      "  -0.29794236]\n",
      " [-0.60756614 -0.75565938 -0.40669155 ... -0.6859925  -0.45223282\n",
      "   0.33418921]]\n"
     ]
    }
   ],
   "source": [
    "print(X_scaled, y_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tEY0B9epqOfq",
    "outputId": "3b56bd6f-82f1-4dc9-f50b-6d6129052e29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28) (10000, 7)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_scaled), np.shape(y_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "uFCxG7FfqZU0",
    "outputId": "b7984268-208b-4623-9f51-6474b08f8e66"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.hist(X[:,0],color ='r')\n",
    "plt.savefig('histogram_setMFX2I01.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "id": "-j2S6mm9qqsw",
    "outputId": "c57418b3-6a22-489c-b6e1-011de0e5a30b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.hist(y[:,1],color ='b')\n",
    "plt.savefig('histogram_normemity.png')\n",
    "#from google.colab import files\n",
    "#files.download( \"histogram_normemity.png\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ad3qzCyznUaf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "split=75\n",
    "# Step 1: Split data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size = (100 - split) / 100.,random_state = 42)\n",
    "# Step 2: Further split the training set into training and validation sets (75% training, 25% validation)\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# learning_rates = [0.01, 0.001, 0.001] #,0.01, 0.001]# 0.01] # 0.001, 0.0001]\n",
    "## batch_sizes = [250, 300, 450, 500, 600] # 128,256]\n",
    "> epochs =  [250, 500, 750, 1000]#, 500]# 1000]\n",
    "> beta_1_values = [0.85, 0.9, 0.95, 0.99]#, 0.9] # 0.9]#, 0.95]\n",
    "> beta_2_values = [0.9, 0.99, 0.999, 0.9999]#, 0.999, 0.9999]\n",
    "\n",
    "> best_loss = float('inf')\n",
    "> best_params = {}\n",
    "\n",
    "> for lr in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        for epoch in epochs:\n",
    "            for beta_1 in beta_1_values:\n",
    "                for beta_2 in beta_2_values:\n",
    "                    for num_neurons in [30, 40, 50, 60, 70,80]: \n",
    "                        optimizer = keras.optimizers.Adam(learning_rate=lr, beta_1=beta_1, beta_2=beta_2)\n",
    "                        model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "                        hist = model.fit(X_train, y_train, epochs=epoch, batch_size=batch_size, validation_data=(X_test, y_test), shuffle=True, > > > verbose=False)\n",
    "                        if hist.history['val_loss'][-1] < best_loss:\n",
    "                            best_loss = hist.history['val_loss'][-1]\n",
    "                            best_params = {'num_neurons':num_neurons,\n",
    "                            'learning_rate': lr,\n",
    "                            'batch_size': batch_size,\n",
    "                            'epochs': epoch,\n",
    "                            'beta_1': beta_1,\n",
    "                            'beta_2': beta_2\n",
    "                                   }\n",
    "\n",
    "> print(\"Best hyperparameters:\", best_params)\n",
    "> import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "# Define the model architecture\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Add four hidden layers, each with 60 nodes and hyperbolic tangent activation function.\n",
    "model.add(layers.Dense(32, activation='tanh', input_shape=(28,)))\n",
    "model.add(layers.Dense(64, activation='tanh'))\n",
    "model.add(layers.Dense(64, activation='tanh'))\n",
    "model.add(layers.Dense(64, activation='tanh'))\n",
    "model.add(layers.Dense(64, activation='tanh'))\n",
    "model.add(layers.Dense(32, activation='tanh'))\n",
    "\n",
    "# Add the output layer with 7 nodes (one for each objective)\n",
    "model.add(layers.Dense(7))\n",
    "\n",
    "# Compile the model with Adam optimizer and appropriate learning rate\n",
    "#optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.99, beta_2=0.999)\n",
    "#model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "\n",
    "# Train the model for 10,000 epochs with a batch size of 500 points\n",
    "#hist1= model.fit(X_train, y_train, epochs=250, batch_size=500, validation_data=(X_test, y_test), shuffle = 'true',verbose = 'false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 18:46:32.100147: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2024-04-09 18:46:32.100190: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2024-04-09 18:46:32.100595: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2024-04-09 18:46:32.100950: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-04-09 18:46:32.100977: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 18:46:32.978656: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 18:46:35.726251: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000\n",
      "Epoch 4/1000\n",
      "Epoch 5/1000\n",
      "Epoch 6/1000\n",
      "Epoch 7/1000\n",
      "Epoch 8/1000\n",
      "Epoch 9/1000\n",
      "Epoch 10/1000\n",
      "Epoch 11/1000\n",
      "Epoch 12/1000\n",
      "Epoch 13/1000\n",
      "Epoch 14/1000\n",
      "Epoch 15/1000\n",
      "Epoch 16/1000\n",
      "Epoch 17/1000\n",
      "Epoch 18/1000\n",
      "Epoch 19/1000\n",
      "Epoch 20/1000\n",
      "Epoch 21/1000\n",
      "Epoch 22/1000\n",
      "Epoch 23/1000\n",
      "Epoch 24/1000\n",
      "Epoch 25/1000\n",
      "Epoch 26/1000\n",
      "Epoch 27/1000\n",
      "Epoch 28/1000\n",
      "Epoch 29/1000\n",
      "Epoch 30/1000\n",
      "Epoch 31/1000\n",
      "Epoch 32/1000\n",
      "Epoch 33/1000\n",
      "Epoch 34/1000\n",
      "Epoch 35/1000\n",
      "Epoch 36/1000\n",
      "Epoch 37/1000\n",
      "Epoch 38/1000\n",
      "Epoch 39/1000\n",
      "Epoch 40/1000\n",
      "Epoch 41/1000\n",
      "Epoch 42/1000\n",
      "Epoch 43/1000\n",
      "Epoch 44/1000\n",
      "Epoch 45/1000\n",
      "Epoch 46/1000\n",
      "Epoch 47/1000\n",
      "Epoch 48/1000\n",
      "Epoch 49/1000\n",
      "Epoch 50/1000\n",
      "Epoch 51/1000\n",
      "Epoch 52/1000\n",
      "Epoch 53/1000\n",
      "Epoch 54/1000\n",
      "Epoch 55/1000\n",
      "Epoch 56/1000\n",
      "Epoch 57/1000\n",
      "Epoch 58/1000\n",
      "Epoch 59/1000\n",
      "Epoch 60/1000\n",
      "Epoch 61/1000\n",
      "Epoch 62/1000\n",
      "Epoch 63/1000\n",
      "Epoch 64/1000\n",
      "Epoch 65/1000\n",
      "Epoch 66/1000\n",
      "Epoch 67/1000\n",
      "Epoch 68/1000\n",
      "Epoch 69/1000\n",
      "Epoch 70/1000\n",
      "Epoch 71/1000\n",
      "Epoch 72/1000\n",
      "Epoch 73/1000\n",
      "Epoch 74/1000\n",
      "Epoch 75/1000\n",
      "Epoch 76/1000\n",
      "Epoch 77/1000\n",
      "Epoch 78/1000\n",
      "Epoch 79/1000\n",
      "Epoch 80/1000\n",
      "Epoch 81/1000\n",
      "Epoch 82/1000\n",
      "Epoch 83/1000\n",
      "Epoch 84/1000\n",
      "Epoch 85/1000\n",
      "Epoch 86/1000\n",
      "Epoch 87/1000\n",
      "Epoch 88/1000\n",
      "Epoch 89/1000\n",
      "Epoch 90/1000\n",
      "Epoch 91/1000\n",
      "Epoch 92/1000\n",
      "Epoch 93/1000\n",
      "Epoch 94/1000\n",
      "Epoch 95/1000\n",
      "Epoch 96/1000\n",
      "Epoch 97/1000\n",
      "Epoch 98/1000\n",
      "Epoch 99/1000\n",
      "Epoch 100/1000\n",
      "Epoch 101/1000\n",
      "Epoch 102/1000\n",
      "Epoch 103/1000\n",
      "Epoch 104/1000\n",
      "Epoch 105/1000\n",
      "Epoch 106/1000\n",
      "Epoch 107/1000\n",
      "Epoch 108/1000\n",
      "Epoch 109/1000\n",
      "Epoch 110/1000\n",
      "Epoch 111/1000\n",
      "Epoch 112/1000\n",
      "Epoch 113/1000\n",
      "Epoch 114/1000\n",
      "Epoch 115/1000\n",
      "Epoch 116/1000\n",
      "Epoch 117/1000\n",
      "Epoch 118/1000\n",
      "Epoch 119/1000\n",
      "Epoch 120/1000\n",
      "Epoch 121/1000\n",
      "Epoch 122/1000\n",
      "Epoch 123/1000\n",
      "Epoch 124/1000\n",
      "Epoch 125/1000\n",
      "Epoch 126/1000\n",
      "Epoch 127/1000\n",
      "Epoch 128/1000\n",
      "Epoch 129/1000\n",
      "Epoch 130/1000\n",
      "Epoch 131/1000\n",
      "Epoch 132/1000\n",
      "Epoch 133/1000\n",
      "Epoch 134/1000\n",
      "Epoch 135/1000\n",
      "Epoch 136/1000\n",
      "Epoch 137/1000\n",
      "Epoch 138/1000\n",
      "Epoch 139/1000\n",
      "Epoch 140/1000\n",
      "Epoch 141/1000\n",
      "Epoch 142/1000\n",
      "Epoch 143/1000\n",
      "Epoch 144/1000\n",
      "Epoch 145/1000\n",
      "Epoch 146/1000\n",
      "Epoch 147/1000\n",
      "Epoch 148/1000\n",
      "Epoch 149/1000\n",
      "Epoch 150/1000\n",
      "Epoch 151/1000\n",
      "Epoch 152/1000\n",
      "Epoch 153/1000\n",
      "Epoch 154/1000\n",
      "Epoch 155/1000\n",
      "Epoch 156/1000\n",
      "Epoch 157/1000\n",
      "Epoch 158/1000\n",
      "Epoch 159/1000\n",
      "Epoch 160/1000\n",
      "Epoch 161/1000\n",
      "Epoch 162/1000\n",
      "Epoch 163/1000\n",
      "Epoch 164/1000\n",
      "Epoch 165/1000\n",
      "Epoch 166/1000\n",
      "Epoch 167/1000\n",
      "Epoch 168/1000\n",
      "Epoch 169/1000\n",
      "Epoch 170/1000\n",
      "Epoch 171/1000\n",
      "Epoch 172/1000\n",
      "Epoch 173/1000\n",
      "Epoch 174/1000\n",
      "Epoch 175/1000\n",
      "Epoch 176/1000\n",
      "Epoch 177/1000\n",
      "Epoch 178/1000\n",
      "Epoch 179/1000\n",
      "Epoch 180/1000\n",
      "Epoch 181/1000\n",
      "Epoch 182/1000\n",
      "Epoch 183/1000\n",
      "Epoch 184/1000\n",
      "Epoch 185/1000\n",
      "Epoch 186/1000\n",
      "Epoch 187/1000\n",
      "Epoch 188/1000\n",
      "Epoch 189/1000\n",
      "Epoch 190/1000\n",
      "Epoch 191/1000\n",
      "Epoch 192/1000\n",
      "Epoch 193/1000\n",
      "Epoch 194/1000\n",
      "Epoch 195/1000\n",
      "Epoch 196/1000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "# Define the model architecture\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Add four hidden layers, each with 60 nodes and hyperbolic tangent activation function.\n",
    "model.add(layers.Dense(40, activation='tanh', input_shape=(28,)))\n",
    "model.add(layers.Dense(40, activation='tanh'))\n",
    "model.add(layers.Dense(40, activation='tanh'))\n",
    "model.add(layers.Dense(40, activation='tanh'))\n",
    "# Add the output layer with 7 nodes (one for each objective)\n",
    "model.add(layers.Dense(7))\n",
    "# Add the output layer with 7 nodes (one for each objective)\n",
    "\n",
    "# Compile the model with Adam optimizer and appropriate learning rate\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.99, beta_2=0.999)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "#Train the model for 10,000 epochs with a batch size of 500 points\n",
    "hist1= model.fit(X_train, y_train, epochs=1000, batch_size=100, validation_data=(X_test, y_test), shuffle = 'true',verbose = 'false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vAS4jipD1lYx"
   },
   "outputs": [],
   "source": [
    "epoch1 = hist1.epoch\n",
    "val_loss1 = hist1.history['val_loss']\n",
    "train_loss1 = hist1.history['loss']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model for 10,000 epochs with a batch size of 500 points\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "# Define the model architecture\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Add four hidden layers, each with 60 nodes and hyperbolic tangent activation function.\n",
    "model.add(layers.Dense(40, activation='tanh', input_shape=(28,)))\n",
    "model.add(layers.Dense(40, activation='tanh'))\n",
    "model.add(layers.Dense(40, activation='tanh'))\n",
    "model.add(layers.Dense(40, activation='tanh'))\n",
    "# Add the output layer with 7 nodes (one for each objective)\n",
    "model.add(layers.Dense(7))\n",
    "# Add the output layer with 7 nodes (one for each objective)\n",
    "\n",
    "# Compile the model with Adam optimizer and appropriate learning rate\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.99, beta_2=0.999)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "#Train the model for 10,000 epochs with a batch size of 500 points\n",
    "hist2= model.fit(X_train, y_train, epochs=1000, batch_size=250, validation_data=(X_test, y_test), shuffle = 'true',verbose = 'false')\n",
    "epoch2 = hist2.epoch\n",
    "val_loss2 = hist2.history['val_loss']\n",
    "train_loss2 = hist2.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model for 10,000 epochs with a batch size of 500 points\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "# Define the model architecture\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Add four hidden layers, each with 60 nodes and hyperbolic tangent activation function.\n",
    "model.add(layers.Dense(40, activation='tanh', input_shape=(28,)))\n",
    "model.add(layers.Dense(40, activation='tanh'))\n",
    "model.add(layers.Dense(40, activation='tanh'))\n",
    "model.add(layers.Dense(40, activation='tanh'))\n",
    "# Add the output layer with 7 nodes (one for each objective)\n",
    "model.add(layers.Dense(7))\n",
    "# Add the output layer with 7 nodes (one for each objective)\n",
    "\n",
    "# Compile the model with Adam optimizer and appropriate learning rate\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.99, beta_2=0.999)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "#Train the model for 10,000 epochs with a batch size of 500 points\n",
    "hist3= model.fit(X_train, y_train, epochs=2000, batch_size=500, validation_data=(X_test, y_test), shuffle = 'true',verbose = 'false')\n",
    "epoch3 = hist3.epoch\n",
    "val_loss3 = hist3.history['val_loss']\n",
    "train_loss3 = hist3.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epoch1, train_loss1, label ='100')\n",
    "plt.plot(epoch2, train_loss2, label ='250')\n",
    "plt.plot(epoch3, train_loss3, label ='500')\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('training  loss')\n",
    "plt.title('training  loss vs epoch for different batch sizes')\n",
    "plt.tight_layout()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epoch1, val_loss1, label ='100')\n",
    "plt.plot(epoch2, val_loss2, label ='250')\n",
    "plt.plot(epoch3, val_loss3, label ='500')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('validation  loss')\n",
    "plt.title('validation  loss vs epoch for different batch sizes')\n",
    "plt.legend()\n",
    "# Show the plot with custom fonts and colors\n",
    "#img = plt.imread('lossvsepoch5.png')\n",
    "\n",
    "# Get the dimensions of the image\n",
    "#height, width, _ = img.shape\n",
    "#plt.title('Subplot 6')\n",
    "# Adjust spacing between subplots\n",
    "#plt.tight_layout()\n",
    "#plt.tight_layout()\n",
    "plt.savefig('simulationvsprediction.png', bbox_inches='tight', pad_inches=0.2)\n",
    "plt.show()\n",
    "# Show the plot\n",
    "#plt.savefig('lossvsepoch5.png')\n",
    "#from google.colab import files\n",
    "#files.download( \"lossvsepoch2.png\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Increase the figure size and font size\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "# Subplot 1\n",
    "plt.subplot(1, 2, 1)\n",
    "#plt.plot(epoch1, train_loss1, label='Batch Size: 100', linewidth=2)  # Increase line width\n",
    "#plt.plot(epoch2, train_loss2, label='Batch Size: 250', linewidth=2)  # Increase line width\n",
    "plt.plot(epoch3, train_loss3, label='Batch Size: 500', color ='red', linewidth=2)  # Increase line width\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Training Loss vs Epoch')\n",
    "\n",
    "# Subplot 2\n",
    "plt.subplot(1, 2, 2)\n",
    "#plt.plot(epoch1, val_loss1, label='Batch Size: 100', linewidth=2)  # Increase line width\n",
    "#plt.plot(epoch2, val_loss2, label='Batch Size: 250', linewidth=2)  # Increase line width\n",
    "plt.plot(epoch3, val_loss3, label='Batch Size: 500', linewidth=2)  # Increase line width\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.title('Validation Loss vs Epoch')\n",
    "plt.legend()\n",
    "\n",
    "# Ensure tight layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot with tight layout\n",
    "plt.savefig('lossvsepoch.png', bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Increase the figure size and font size\n",
    "plt.figure(figsize=(15, 10))  # Increase the figure size\n",
    "plt.rcParams.update({'font.size': 16})  # Increase font size\n",
    "\n",
    "# Subplot 1\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epoch1, train_loss1, label='Batch Size: 100', linewidth=3)  # Increase line width\n",
    "plt.plot(epoch2, train_loss2, label='Batch Size: 250', linewidth=3)  # Increase line width\n",
    "plt.plot(epoch3, train_loss3, label='Batch Size: 500', linewidth=3)  # Increase line width\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch', fontsize=18)  # Increase xlabel font size\n",
    "plt.ylabel('Training Loss', fontsize=18)  # Increase ylabel font size\n",
    "plt.title('Training Loss vs Epoch for Different Batch Sizes', fontsize=20)  # Increase title font size\n",
    "\n",
    "# Subplot 2\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epoch1, val_loss1, label='Batch Size: 100', linewidth=3)  # Increase line width\n",
    "plt.plot(epoch2, val_loss2, label='Batch Size: 250', linewidth=3)  # Increase line width\n",
    "plt.plot(epoch3, val_loss3, label='Batch Size: 500', linewidth=3)  # Increase line width\n",
    "plt.xlabel('Epoch', fontsize=18)  # Increase xlabel font size\n",
    "plt.ylabel('Validation Loss', fontsize=18)  # Increase ylabel font size\n",
    "plt.title('Validation Loss vs Epoch for Different Batch Sizes', fontsize=20)  # Increase title font size\n",
    "plt.legend()\n",
    "\n",
    "# Ensure tight layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot with tight layout\n",
    "plt.savefig('lossvsepoch2.png', bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rnXC0TfF5XqS",
    "outputId": "eb9d1825-8b39-4ed6-cb0e-4165283fec74"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "# Define the model architecture\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Add four hidden layers, each with 60 nodes and hyperbolic tangent activation function.\n",
    "model.add(layers.Dense(60, activation='tanh', input_shape=(28,)))\n",
    "model.add(layers.Dense(60, activation='tanh'))\n",
    "model.add(layers.Dense(60, activation='tanh'))\n",
    "model.add(layers.Dense(60, activation='tanh'))\n",
    "\n",
    "# Add the output layer with 7 nodes (one for each objective)\n",
    "model.add(layers.Dense(7))\n",
    "\n",
    "# Compile the model with Adam optimizer and appropriate learning rate\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.99, beta_2=0.999)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "\n",
    "# Train the model for 10,000 epochs with a batch size of 500 points\n",
    "hist4= model.fit(X_train, y_train, epochs=500, batch_size=250, validation_data=(X_test, y_test), shuffle = 'true',verbose = 'false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2g9iS-my5-IK"
   },
   "outputs": [],
   "source": [
    "epoch4 = hist4.epoch\n",
    "val_loss4 = hist4.history['val_loss']\n",
    "train_loss4 = hist4.history['loss']\n",
    "#print(epoch, val_loss1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yVblTaW76MEH",
    "outputId": "ce92cc4a-f34c-4dd3-8c81-03b9a7126960"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "# Define the model architecture\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Add four hidden layers, each with 60 nodes and hyperbolic tangent activation function.\n",
    "# Add four hidden layers, each with 60 nodes and hyperbolic tangent activation function.\n",
    "model.add(layers.Dense(60, activation='tanh', input_shape=(28,)))\n",
    "model.add(layers.Dense(60, activation='tanh'))\n",
    "model.add(layers.Dense(60, activation='tanh'))\n",
    "model.add(layers.Dense(60, activation='tanh'))\n",
    "\n",
    "# Add the output layer with 7 nodes (one for each objective)\n",
    "model.add(layers.Dense(7))\n",
    "\n",
    "# Compile the model with Adam optimizer and appropriate learning rate\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "\n",
    "# Train the model for 10,000 epochs with a batch size of 500 poin\n",
    "\n",
    "hist5= model.fit(X_train, y_train, epochs=500, batch_size=500, validation_data=(X_test, y_test), shuffle = 'true',verbose = 'false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j3DD7zEZ6W1v"
   },
   "outputs": [],
   "source": [
    "epoch5 = hist5.epoch\n",
    "val_loss5 = hist5.history['val_loss']\n",
    "train_loss5 = hist5.history['loss']\n",
    "#print(epoch, val_loss1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 806
    },
    "id": "wkHLN0OP61HP",
    "outputId": "83970fc7-26af-468f-8c92-c595c9db3ec4"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epoch4, train_loss4, label ='250')\n",
    "plt.plot(epoch5, train_loss5, label ='500')\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('training  loss')\n",
    "plt.title('training  loss vs epoch for different batch sizes')\n",
    "plt.tight_layout()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epoch4, val_loss4, label ='250')\n",
    "plt.plot(epoch5, val_loss5, label ='500')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('validation  loss')\n",
    "plt.title('validation  loss vs epoch for different batch sizes')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show\n",
    "plt.savefig('lossvsepoch21.png')\n",
    "#from google.colab import files\n",
    "#files.download( \"lossvsepoch2.png\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wDLI0Kk481yS",
    "outputId": "079e0ddb-7fff-4456-9329-1cd82892b5da"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x=X_test)\n",
    "row,col= y_pred.shape\n",
    "print(row, col)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1mJfpzNxuBqT"
   },
   "outputs": [],
   "source": [
    "# Assuming 'y_pred_scaled' contains the scaled predictions from the neural network\n",
    "#X_original = scaler.inverse_transform(X_scaled)\n",
    "# To get back to the original data from the scaled data\n",
    "#x_original = scaler_x.inverse_transform(x_scaled)\n",
    "#y_original = scaler_y.inverse_transform(y_scaled)\n",
    "y_pred_original_scale =scaler_y.inverse_transform(y_pred)\n",
    "y_test_original_scale = scaler_y.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_original_scale.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "row,col= y_pred_original_scale.shape\n",
    "print(row, col)\n",
    "plt.figure(figsize=(15,15))\n",
    "ind =int(np.sqrt(col))+1\n",
    "for i in range(0,col):\n",
    "    plt.subplot(ind,ind,i+1)\n",
    "    plt.hexbin(y_test_original_scale [:,i], y_pred_original_scale [:,i], cmap='Reds') #supported values are 'Accent', 'Accent_r', 'Blues', 'Blues_r', 'BrBG', \n",
    "    #'BrBG_r', 'BuGn', 'BuGn_r', 'BuPu', 'BuPu_r', 'CMRmap', 'CMRmap_r', 'Dark2', 'Dark2_r',\n",
    "    #'GnBu', 'GnBu_r', 'Greens', 'Greens_r', 'Greys', 'Greys_r', 'OrRd', 'OrRd_r', 'Oranges', 'Oranges_r', '\n",
    "    #PRGn', 'PRGn_r', 'Paired', 'Paired_r', 'Pastel1', 'Pastel1_r', 'Pastel2', 'Pastel2_r', 'PiYG', 'PiYG_r', 'PuBu', 'PuBuGn',\n",
    "    #'PuBuGn_r', 'PuBu_r', 'PuOr', 'PuOr_r', 'PuRd', 'PuRd_r', 'Purples', 'Purples_r', 'RdBu', 'RdBu_r', 'RdGy', 'RdGy_r', \n",
    "    #'RdPu', 'RdPu_r', 'RdYlBu', 'RdYlBu_r', 'RdYlGn', 'RdYlGn_r', 'Reds', 'Reds_r', 'Set1', 'Set1_r', 'Set2', 'Set2_r', 'Set3', \n",
    "    #'Set3_r', 'Spectral', 'Spectral_r', 'Wistia', 'Wistia_r', 'YlGn', 'YlGnBu', 'YlGnBu_r', 'YlGn_r', 'YlOrBr', 'YlOrBr_r', 'YlOrRd', 'YlOrRd_r', 'afmhot', 'afmhot_r', 'autumn', 'autumn_r', 'binary', 'binary_r', 'bone', 'bone_r', 'brg', 'brg_r', 'bwr', 'bwr_r', 'cividis', 'cividis_r', 'cool', 'cool_r', 'coolwarm', 'coolwarm_r', 'copper', 'copper_r', 'cubehelix', 'cubehelix_r', 'flag', 'flag_r', 'gist_earth', 'gist_earth_r', 'gist_gray', 'gist_gray_r', 'gist_heat', 'gist_heat_r', 'gist_ncar', 'gist_ncar_r', 'gist_rainbow', 'gist_rainbow_r', 'gist_stern', 'gist_stern_r', 'gist_yarg', 'gist_yarg_r', 'gnuplot', 'gnuplot2', 'gnuplot2_r', 'gnuplot_r', 'gray', 'gray_r', 'hot', 'hot_r', 'hsv', 'hsv_r', 'inferno', 'inferno_r', 'jet', 'jet_r', 'magma', 'magma_r', 'nipy_spectral', 'nipy_spectral_r', 'ocean', 'ocean_r', 'pink', 'pink_r', 'plasma', 'plasma_r', 'prism', 'prism_r', 'rainbow', 'rainbow_r', 'seismic', 'seismic_r', 'spring', 'spring_r', 'summer', 'summer_r', 'tab10', 'tab10_r', 'tab20', 'tab20_r', 'tab20b', 'tab20b_r', 'tab20c', 'tab20c_r', 'terrain', 'terrain_r', 'turbo', 'turbo_r', 'twilight', 'twilight_r', 'twilight_shifted', 'twilight_shifted_r', 'viridis', 'viridis_r', 'winter', 'winter_r'\n",
    "    plt.ylabel('predicted outputs')\n",
    "    plt.xlabel('actual outputs')\n",
    "    plt.tight_layout()\n",
    "plt.show()\n",
    "residuals= (y_pred_original_scale - y_test_original_scale)\n",
    "rsq = np.sum(residuals**2, axis =0)/(np.sum(y_test_original_scale**2, axis=0))\n",
    "print(rsq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(y_test_original_scale [:,0]*1e+7, y_pred_original_scale [:,0]*1e+7, '.r')      \n",
    "plt.ylabel('$\\epsilon_{n,x}$(test) (mm-mrad)')\n",
    "plt.xlabel('$\\epsilon_{n,x}$(predicted) (mm-mrad)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#residuals= (y_pred_original_scale[:,0]- y_test_original_scale[:,0])\n",
    "#rsq = np.sum(residuals**2, axis =0)/(np.sum(y_test_original_scale**2, axis=0))\n",
    "#print(rsq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.figure(figsize=(10, 10))  # Adjust the figure size as needed\n",
    "# First subplot\n",
    "plt.subplot(3, 3, 1)\n",
    "plt.plot(y_test_original_scale [:,0]*1e+6, y_pred_original_scale [:,0]*1e+6, '.r')      \n",
    "plt.ylabel('$\\epsilon_{n,x}$(test) (mm-mrad)')\n",
    "plt.xlabel('$\\epsilon_{n,x}$(predicted) (mm-mrad)')\n",
    "#plt.title('Subplot 1')\n",
    "# Second subplot\n",
    "plt.subplot(3, 3, 2)\n",
    "plt.plot(y_test_original_scale [:,1]*1e+6, y_pred_original_scale [:,1]*1e+6, '.b')      \n",
    "plt.ylabel('$\\epsilon_{n,y}$(test) (mm-mrad)')\n",
    "plt.xlabel('$\\epsilon_{n,y}$(predicted) (mm-mrad)')\n",
    "# Third subplot\n",
    "plt.subplot(3, 3, 3)\n",
    "plt.plot(y_test_original_scale [:,2]*1e+12, y_pred_original_scale [:,2]*1e+12, '.g')      \n",
    "plt.ylabel('$\\sigma_z$(test) (ps)')\n",
    "plt.xlabel('$\\sigma_z$ (predicted) (ps)')\n",
    "#plt.title('Subplot 3')\n",
    "# Fourth subplot\n",
    "plt.subplot(3, 3, 4)\n",
    "plt.plot(y_test_original_scale [:,3]*1e+2, y_pred_original_scale [:,3]*1e+2, '.r')      \n",
    "plt.ylabel('beam-loss (mm-mrad)')\n",
    "plt.xlabel('beam-loss (predicted) (mm-mrad)')\n",
    "#plt.title('Subplot 4')\n",
    "# Fifth subplot\n",
    "plt.subplot(3, 3, 5)\n",
    "plt.plot(y_test_original_scale [:,4]*1e+3, y_pred_original_scale [:,4]*1e+3, '.g')      \n",
    "plt.ylabel('$\\sigma_{x}$(test) (mm)')\n",
    "plt.xlabel('$\\sigma_x$(predicted) (mm)')\n",
    "#plt.title('Subplot 5')\n",
    "# Sixth subplot\n",
    "plt.subplot(3, 3, 6)\n",
    "plt.plot(y_test_original_scale [:,5]*1e+3, y_pred_original_scale [:,5]*1e+3, '.b')      \n",
    "plt.ylabel('$\\sigma_{y}$(test) (mm)')\n",
    "plt.xlabel('$\\sigma_y$(predicted) (mm)')\n",
    "#plt.title('Subplot 6')\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(y_test_original_scale [:,6]/(7*1e+6), y_pred_original_scale [:,6]/(7*1e+6), '.g')      \n",
    "plt.ylabel('$\\sigma_{E_k}$(test) ')\n",
    "plt.xlabel('$\\sigma_{E_k}$(predicted)')\n",
    "#plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# Assuming y_test_original_scale and y_pred_original_scale are your data arrays\n",
    "\n",
    "# Define the linear function for LSQ fit (y = mx + b)\n",
    "def linear_fit(x, m, b):\n",
    "    return m * x + b\n",
    "\n",
    "# Perform the LSQ fit\n",
    "popt, _ = curve_fit(linear_fit, y_test_original_scale[:, 6] / 7000000, y_pred_original_scale[:, 6] / 7000000)\n",
    "\n",
    "# Extract the fit coefficients\n",
    "m, b = popt\n",
    "\n",
    "# Generate the predicted values using the LSQ fit\n",
    "y_fit = linear_fit(y_test_original_scale[:, 6] / 7000000, m, b)\n",
    "\n",
    "# Create a figure\n",
    "plt.figure(figsize=(5, 4))\n",
    "\n",
    "# Plot the original data\n",
    "plt.plot(y_test_original_scale[:, 6] / 7000000, y_pred_original_scale[:, 6] / 7000000, '.g', label='Data')\n",
    "\n",
    "# Plot the LSQ fit line\n",
    "plt.plot(y_test_original_scale[:, 6] / 7000000, y_fit, 'r-', label='LSQ Fit')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.ylabel('$\\sigma_{E_k}$(test)')\n",
    "plt.xlabel('$\\sigma_{E_k}$(predicted)')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# Assuming y_test_original_scale and y_pred_original_scale are your data arrays\n",
    "\n",
    "# Define the linear function for LSQ fit (y = mx + b)\n",
    "def linear_fit(x, m, b):\n",
    "    return m * x + b\n",
    "\n",
    "# Perform the LSQ fit\n",
    "popt, _ = curve_fit(linear_fit, y_test_original_scale[:, 6] / 7000000, y_pred_original_scale[:, 6] / 7000000)\n",
    "\n",
    "# Extract the fit coefficients\n",
    "m, b = popt\n",
    "\n",
    "# Generate the predicted values using the LSQ fit\n",
    "y_fit = linear_fit(y_test_original_scale[:, 6] / 7000000, m, b)\n",
    "\n",
    "# Create a figure\n",
    "plt.figure(figsize=(6,6 ))\n",
    "\n",
    "# Plot the original data with a scatter plot\n",
    "plt.scatter(y_test_original_scale[:, 6] / 7000000, y_pred_original_scale[:, 6] / 7000000, c='g',  marker='.')\n",
    "\n",
    "# Plot the LSQ fit line with a bold, large line\n",
    "plt.plot(y_test_original_scale[:, 6] / 7000000, y_fit, 'r-', linewidth=3)\n",
    "\n",
    "# Add labels and legend\n",
    "plt.ylabel('$\\sigma_{E_k}$(test)')\n",
    "plt.xlabel('$\\sigma_{E_k}$(predicted)')\n",
    "#plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt  # Import for plotting\n",
    "\n",
    "# Define the model architecture\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Add four hidden layers, each with 60 nodes and hyperbolic tangent activation function.\n",
    "model.add(layers.Dense(40, activation='tanh', input_shape=(28,)))\n",
    "model.add(layers.Dense(40, activation='tanh'))\n",
    "model.add(layers.Dense(40, activation='tanh'))\n",
    "model.add(layers.Dense(40, activation='tanh'))\n",
    "# Add the output layer with 7 nodes (one for each objective)\n",
    "model.add(layers.Dense(7))\n",
    "\n",
    "# Compile the model with Adam optimizer and appropriate learning rate\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.99, beta_2=0.999)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Create empty lists to store MSE loss values\n",
    "train_losses = []\n",
    "\n",
    "# Train the model for 1000 epochs with a batch size of 100 points\n",
    "for epoch in range(2000):\n",
    "    history = model.fit(X_train, y_train, epochs=1, batch_size=500, validation_data=(X_test, y_test), shuffle=True, verbose=0)\n",
    "    \n",
    "    # Calculate and append MSE loss to the list\n",
    "    mse_loss = history.history['loss'][0]\n",
    "    train_losses.append(mse_loss)\n",
    "\n",
    "\n",
    "plt.tick_params(axis='both', which='both', direction='in', labelsize=15)  # Increase font size and use ticks inside\n",
    "#plt.legend(loc='upper right', borderpad=1)  # Place legend inside and increase border scale size\n",
    "# Increase border line size, tick size, and font size\n",
    "plt.gca().spines['top'].set_linewidth(1.5)\n",
    "plt.gca().spines['right'].set_linewidth(1.5)\n",
    "plt.gca().spines['bottom'].set_linewidth(1.5)\n",
    "plt.gca().spines['left'].set_linewidth(1.5)\n",
    "plt.tick_params(axis='both', which='major', labelsize=16, width=2)\n",
    "plt.tick_params(axis='both', which='minor', labelsize=16, width=2)\n",
    "# Ensure tight layout\n",
    "\n",
    "# Plot the MSE loss vs. number of training examples seen\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, color='blue')\n",
    "plt.xlabel('Number of Training Examples Seen')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the model architecture\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Add four hidden layers, each with 60 nodes and hyperbolic tangent activation function.\n",
    "model.add(layers.Dense(40, activation='tanh', input_shape=(28,)))\n",
    "model.add(layers.Dense(40, activation='tanh'))\n",
    "model.add(layers.Dense(40, activation='tanh'))\n",
    "model.add(layers.Dense(40, activation='tanh'))\n",
    "# Add the output layer with 7 nodes (one for each objective)\n",
    "model.add(layers.Dense(7))\n",
    "\n",
    "# Compile the model with Adam optimizer and appropriate learning rate\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.99, beta_2=0.999)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Create empty lists to store MSE loss values and number of training examples seen\n",
    "train_losses = []\n",
    "num_training_examples_seen = []\n",
    "\n",
    "# Initialize a counter for tracking the number of training examples seen\n",
    "examples_seen = 0\n",
    "\n",
    "# Train the model for 1000 epochs with a batch size of 100 points\n",
    "for epoch in range(1000):\n",
    "    history = model.fit(X_train, y_train, epochs=1, batch_size=500, validation_data=(X_test, y_test), shuffle=True, verbose=0)\n",
    "    \n",
    "    # Get the number of training examples in the current batch\n",
    "    batch_size = X_train.shape[0]\n",
    "    \n",
    "    # Update the counter for the number of training examples seen\n",
    "    examples_seen += batch_size\n",
    "    \n",
    "    # Calculate and append MSE loss to the list\n",
    "    mse_loss = history.history['loss'][0]\n",
    "    train_losses.append(mse_loss)\n",
    "    \n",
    "    # Append the number of training examples seen to the list\n",
    "    num_training_examples_seen.append(examples_seen)\n",
    "\n",
    "# Plot the MSE loss vs. the number of training examples seen\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(num_training_examples_seen, train_losses, color='blue')\n",
    "plt.xlabel('Number of Training Examples Seen')\n",
    "#plt.xlim(200)\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(num_training_examples_seen, train_losses, color='blue')\n",
    "plt.xlabel('Number of Training Examples Seen',fontsize=18)\n",
    "#plt.xlim(200)\n",
    "plt.ylabel('MSE Loss',fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig('mselsossvsnumberof training.png', bbox_inches='tight', pad_inches=0.2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(num_training_examples_seen, train_losses, color='blue')\n",
    "plt.xlabel('Number of Training Examples Seen')\n",
    "plt.xlim([300,500000])\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.figure(figsize=(14.8, 10))  # Adjust the figure size as needed\n",
    "# First subplot\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(y_test_original_scale[:,0]*1e+6, y_test_original_scale[:,2]*1e+12, '.r', label='GPT simulation')  \n",
    "plt.plot(y_pred_original_scale[:,0]*1e+6, y_pred_original_scale[:,2]*1e+12, '.b', label ='NN Prediction')  \n",
    "plt.xlabel('$\\epsilon_{n,x}$ (mm-mrad)')\n",
    "plt.ylabel('$\\sigma_{z}$ (ps)')\n",
    "plt.legend()\n",
    "#plt.title('Subplot 1')\n",
    "# Second subplot\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(y_test_original_scale[:,1]*1e+6, y_test_original_scale[:,2]*1e+12, '.r', label='GPT simulation')  \n",
    "plt.plot(y_pred_original_scale[:,1]*1e+6, y_pred_original_scale[:,2]*1e+12, '.b', label ='NN Prediction')  \n",
    "plt.xlabel('$\\epsilon_{n,y}$ (mm-mrad)')\n",
    "plt.ylabel('$\\sigma_{z}$ (ps)')\n",
    "plt.legend()\n",
    "# Fifth subplot\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(y_test_original_scale[:,6]/7000000, y_test_original_scale[:,4]*1e+3, '.g', label='GPT simulation')  \n",
    "plt.plot(y_pred_original_scale[:,6]/7000000, y_pred_original_scale[:,4]*1e+3, '.m', label ='NN Prediction')     \n",
    "plt.xlabel('$\\sigma_{\\mathrm{E_k}}$ (ps)')\n",
    "plt.ylabel('$\\sigma_x$( (mm)')\n",
    "plt.legend()\n",
    "#plt.title('Subplot 5')\n",
    "# Sixth subplot\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(y_test_original_scale[:,2]*1e+12, y_test_original_scale[:,4]*1000, '.r', label='GPT simulation')  \n",
    "plt.plot(y_pred_original_scale[:,2]*1e+12, y_pred_original_scale[:,4]*1000, '.b', label ='NN Prediction')  \n",
    "plt.ylabel('$\\sigma_{x}$ (mm)')\n",
    "plt.xlabel('$\\sigma_{z}$ (ps)')\n",
    "plt.legend()\n",
    "# Show the plot with custom fonts and colors\n",
    "img = plt.imread('simulationvsprediction.png')\n",
    "\n",
    "# Get the dimensions of the image\n",
    "height, width, _ = img.shape\n",
    "#plt.title('Subplot 6')\n",
    "# Adjust spacing between subplots\n",
    "#plt.tight_layout()\n",
    "#plt.tight_layout()\n",
    "plt.savefig('simulationvsprediction.png', bbox_inches='tight', pad_inches=0.2)\n",
    "plt.show()\n",
    "# Show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_nn_model():\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(40, activation='tanh', input_shape=(28,)))\n",
    "    model.add(layers.Dense(40, activation='tanh'))\n",
    "    model.add(layers.Dense(40, activation='tanh'))\n",
    "    model.add(layers.Dense(40, activation='tanh'))\n",
    "    # Add the output layer with 7 nodes (one for each objective)\n",
    "    model.add(layers.Dense(7))\n",
    "    return model\n",
    "\n",
    "# Define a function to generate and train a neural network model\n",
    "def generate_and_train_model(epochs=100, plot_loss=True):\n",
    "    model = create_nn_model()\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.99, beta_2=0.999)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    # Add your training data (X_train, y_train) here\n",
    "    \n",
    "    loss_list = []\n",
    "    for epoch in range(epochs):\n",
    "        history = model.fit(X_train, y_train, epochs=1, batch_size=200, verbose=0)\n",
    "        loss = history.history['loss'][0]\n",
    "        loss_list.append(loss)\n",
    "    \n",
    "    if plot_loss:\n",
    "        plt.plot(loss_list)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Training Loss')\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ensemble of 10 neural network models\n",
    "ensemble_models = [generate_and_train_model() for _ in range(10)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_nn_model():\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(40, activation='tanh', input_shape=(28,)))\n",
    "    model.add(layers.Dense(40, activation='tanh'))\n",
    "    model.add(layers.Dense(40, activation='tanh'))\n",
    "    model.add(layers.Dense(40, activation='tanh'))\n",
    "    # Add the output layer with 7 nodes (one for each objective)\n",
    "    model.add(layers.Dense(7))\n",
    "    return model\n",
    "\n",
    "# Define a function to generate and train a neural network model\n",
    "def generate_and_train_model(epochs=100, plot_loss=True):\n",
    "    model = create_nn_model()\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.99, beta_2=0.999)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    # Add your training data (X_train, y_train) here\n",
    "    \n",
    "    loss_list = []\n",
    "    for epoch in range(epochs):\n",
    "        history = model.fit(X_train, y_train, epochs=1, batch_size=500, verbose=0)\n",
    "        loss = history.history['loss'][0]\n",
    "        loss_list.append(loss)\n",
    "    \n",
    "    if plot_loss:\n",
    "        plt.plot(loss_list)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Training Loss')\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ensemble of 10 neural network models\n",
    "ensemble_models = [generate_and_train_model() for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=X_scaled,\n",
    "objectives=y_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred= model.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred[10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "# Define the optimization problem\n",
    "creator.create(\"FitnessMulti\", base.Fitness, weights=(-1.0, -1.0))  # Minimize both objectives\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMulti)\n",
    "\n",
    "# Define the bounds for your 28 variables (features)\n",
    "BOUND_LOW = [-1.0] * 28  # Replace with your lower bounds\n",
    "BOUND_UP = [1.0] * 28   # Replace with your upper bounds\n",
    "\n",
    "# Create the DEAP toolbox\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "def random_float(minval, maxval):\n",
    "    return random.uniform(minval, maxval)\n",
    "\n",
    "# Register attribute, individual initialization, and population creation\n",
    "toolbox.register(\"attr_float\", random_float, BOUND_LOW, BOUND_UP)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_float, n=28)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate(individual):\n",
    "    # Extract the features from the individual\n",
    "    features = np.array(individual)\n",
    "    \n",
    "    # Train your neural network and evaluate it\n",
    "    model = create_and_train_neural_network(features)  # Create and train your neural network\n",
    "    objectives = evaluate_neural_network(model)  # Evaluate the neural network\n",
    "    \n",
    "    return objectives\n",
    "\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "\n",
    "# Configure NSGA-II parameters\n",
    "MU = 100  # Number of individuals in each generation\n",
    "LAMBDA = 120  # Number of offspring to produce in each generation\n",
    "CXPB = 0.7  # Probability of mating two individuals\n",
    "MUTPB = 0.2  # Probability of mutating an individual\n",
    "NGEN = 1000  # Number of generations\n",
    "\n",
    "# Create the initial population\n",
    "pop = toolbox.population(n=MU)\n",
    "\n",
    "# Run the NSGA-II algorithm\n",
    "algorithms.eaMuPlusLambda(pop, toolbox, MU, LAMBDA, CXPB, MUTPB, NGEN)\n",
    "\n",
    "# Extract the non-dominated fronts\n",
    "fronts = tools.sortNondominated(pop, len(pop), first_front_only=True)\n",
    "\n",
    "# Extract solutions from front 0 and front 1\n",
    "optimal_solutions_front0 = fronts[0]\n",
    "optimal_solutions_front1 = fronts[1]\n",
    "\n",
    "# Extract corresponding variables (X) and objectives (Y)\n",
    "X_optimal_front0 = np.array([ind for ind in optimal_solutions_front0])\n",
    "X_optimal_front1 = np.array([ind for ind in optimal_solutions_front1])\n",
    "\n",
    "# Evaluate objectives for the optimal solutions\n",
    "Y_optimal_front0 = np.array([evaluate(ind) for ind in optimal_solutions_front0])\n",
    "Y_optimal_front1 = np.array([evaluate(ind) for ind in optimal_solutions_front1])\n",
    "\n",
    "# Print or store the optimal solutions, variables, and objectives as needed\n",
    "print(\"Optimal solutions from front 0:\", X_optimal_front0, Y_optimal_front0)\n",
    "print(\"Optimal solutions from front 1:\", X_optimal_front1, Y_optimal_front1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "# Define the optimization problem\n",
    "creator.create(\"FitnessMulti\", base.Fitness, weights=(-1.0,) * 7)  # Minimize all seven objectives\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMulti)\n",
    "\n",
    "# Define the bounds for your 28 variables (features)\n",
    "BOUND_LOW = [0.0] * 28  # Replace with your lower bounds\n",
    "BOUND_UP = [1.0] * 28   # Replace with your upper bounds\n",
    "\n",
    "# Create the DEAP toolbox\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "def random_float(minval, maxval):\n",
    "    return random.uniform(minval, maxval)\n",
    "\n",
    "# Register attribute, individual initialization, and population creation\n",
    "toolbox.register(\"attr_float\", random_float, BOUND_LOW, BOUND_UP)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_float, n=28)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate(individual):\n",
    "    # Extract the features from the individual\n",
    "    features = np.array(individual)\n",
    "    \n",
    "    # Train your neural network and evaluate it\n",
    "    model = create_and_train_neural_network(features)  # Create and train your neural network\n",
    "    objectives = evaluate_neural_network(model)  # Evaluate the neural network\n",
    "    \n",
    "    return tuple(objectives)  # Return objectives as a tuple\n",
    "\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "\n",
    "# Configure NSGA-II parameters\n",
    "MU = 100  # Number of individuals in each generation\n",
    "LAMBDA = 120  # Number of offspring to produce in each generation\n",
    "CXPB = 0.7  # Probability of mating two individuals\n",
    "MUTPB = 0.2  # Probability of mutating an individual\n",
    "NGEN = 1000  # Number of generations\n",
    "\n",
    "# Create the initial population\n",
    "pop = toolbox.population(n=MU)\n",
    "\n",
    "# Run the NSGA-II algorithm\n",
    "algorithms.eaMuPlusLambda(pop, toolbox, MU, LAMBDA, CXPB, MUTPB, NGEN)\n",
    "\n",
    "# Extract the non-dominated fronts\n",
    "fronts = tools.sortNondominated(pop, len(pop), first_front_only=False)\n",
    "\n",
    "# Extract solutions from each front (including all fronts)\n",
    "optimal_solutions = [front[:] for front in fronts]\n",
    "\n",
    "# Extract corresponding variables (X) and objectives (Y) for each front\n",
    "X_optimal_solutions = [np.array([ind for ind in front]) for front in optimal_solutions]\n",
    "Y_optimal_solutions = [np.array([evaluate(ind) for ind in front]) for front in optimal_solutions]\n",
    "\n",
    "# Print or store the optimal solutions, variables, and objectives for all fronts\n",
    "for i, (X_optimal_front, Y_optimal_front) in enumerate(zip(X_optimal_solutions, Y_optimal_solutions)):\n",
    "    print(f\"Optimal solutions from front {i}:\", X_optimal_front, Y_optimal_front)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "# Define the optimization problem\n",
    "creator.create(\"FitnessMulti\", base.Fitness, weights=(-1.0,) * 7)  # Minimize all seven objectives\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMulti)\n",
    "\n",
    "# Define the bounds for your 28 variables (features)\n",
    "BOUND_LOW = [0.0] * 28  # Replace with your lower bounds\n",
    "BOUND_UP = [1.0] * 28   # Replace with your upper bounds\n",
    "\n",
    "# Create the DEAP toolbox\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "def random_float(minval, maxval):\n",
    "    return random.uniform(minval, maxval)\n",
    "\n",
    "# Register attribute, individual initialization, and population creation\n",
    "toolbox.register(\"attr_float\", random_float, BOUND_LOW, BOUND_UP)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_float, n=28)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate(individual):\n",
    "    # Extract the features from the individual\n",
    "    features = np.array(individual)\n",
    "    \n",
    "    # Train your neural network and evaluate it\n",
    "    model = create_and_train_neural_network(features)  # Create and train your neural network\n",
    "    objectives = evaluate_neural_network(model)  # Evaluate the neural network\n",
    "    \n",
    "    return tuple(objectives)  # Return objectives as a tuple\n",
    "\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "\n",
    "# Configure NSGA-II parameters\n",
    "MU = 100  # Number of individuals in each generation\n",
    "LAMBDA = 120  # Number of offspring to produce in each generation\n",
    "CXPB = 0.7  # Probability of mating two individuals\n",
    "MUTPB = 0.2  # Probability of mutating an individual\n",
    "NGEN = 1000  # Number of generations\n",
    "\n",
    "# Create the initial population\n",
    "pop = toolbox.population(n=MU)\n",
    "\n",
    "# Run the NSGA-II algorithm\n",
    "algorithms.eaMuPlusLambda(pop, toolbox, MU, LAMBDA, CXPB, MUTPB, NGEN)\n",
    "\n",
    "# Extract the non-dominated fronts\n",
    "fronts = tools.sortNondominated(pop, len(pop), first_front_only=False)\n",
    "\n",
    "# Extract solutions from each front (including all fronts)\n",
    "optimal_solutions = [front[:] for front in fronts]\n",
    "\n",
    "# Extract corresponding variables (X) and objectives (Y) for each front\n",
    "X_optimal_solutions = [np.array([ind for ind in front]) for front in optimal_solutions]\n",
    "Y_optimal_solutions = [np.array([evaluate(ind) for ind in front]) for front in optimal_solutions]\n",
    "\n",
    "# Print or store the optimal solutions, variables, and objectives for all fronts\n",
    "for i, (X_optimal_front, Y_optimal_front) in enumerate(zip(X_optimal_solutions, Y_optimal_solutions)):\n",
    "    print(f\"Optimal solutions from front {i}:\", X_optimal_front)\n",
    "    print(f\"Objectives of front {i}:\", Y_optimal_front)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "# Define the optimization problem\n",
    "creator.create(\"FitnessMulti\", base.Fitness, weights=(-1.0,) * 7)  # Minimize all seven objectives\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMulti)\n",
    "\n",
    "# Define the bounds for your 28 variables (features)\n",
    "BOUND_LOW = [0.0] * 28  # Replace with your lower bounds\n",
    "BOUND_UP = [1.0] * 28   # Replace with your upper bounds\n",
    "\n",
    "# Create the DEAP toolbox\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "def random_float(minval, maxval):\n",
    "    return random.uniform(minval, maxval)\n",
    "\n",
    "# Register attribute, individual initialization, and population creation\n",
    "toolbox.register(\"attr_float\", random_float, BOUND_LOW, BOUND_UP)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_float, n=28)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate(individual):\n",
    "    # Extract the features from the individual\n",
    "    features = np.array(individual)\n",
    "    \n",
    "    # Train your neural network and evaluate it\n",
    "    model = create_and_train_neural_network(features)  # Create and train your neural network\n",
    "    objectives = evaluate_neural_network(model)  # Evaluate the neural network\n",
    "    \n",
    "    return objectives  # Objectives are already in the correct format\n",
    "\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "\n",
    "# Configure NSGA-II parameters\n",
    "MU = 100  # Number of individuals in each generation\n",
    "LAMBDA = 120  # Number of offspring to produce in each generation\n",
    "CXPB = 0.7  # Probability of mating two individuals\n",
    "MUTPB = 0.2  # Probability of mutating an individual\n",
    "NGEN = 1000  # Number of generations\n",
    "\n",
    "# Create the initial population\n",
    "pop = toolbox.population(n=MU)\n",
    "\n",
    "# Run the NSGA-II algorithm\n",
    "algorithms.eaMuPlusLambda(pop, toolbox, MU, LAMBDA, CXPB, MUTPB, NGEN)\n",
    "\n",
    "# Extract the non-dominated fronts\n",
    "fronts = tools.sortNondominated(pop, len(pop), first_front_only=False)\n",
    "\n",
    "# Extract solutions from each front (including all fronts)\n",
    "optimal_solutions = [front[:] for front in fronts]\n",
    "\n",
    "# Extract corresponding variables (X) and objectives (Y) for each front\n",
    "X_optimal_solutions = [np.array([ind for ind in front]) for front in optimal_solutions]\n",
    "Y_optimal_solutions = [np.array([evaluate(ind) for ind in front]) for front in optimal_solutions]\n",
    "\n",
    "# Print or store the optimal solutions, variables, and objectives for all fronts\n",
    "for i, (X_optimal_front, Y_optimal_front) in enumerate(zip(X_optimal_solutions, Y_optimal_solutions)):\n",
    "    print(f\"Optimal solutions from front {i}:\", X_optimal_front)\n",
    "    print(f\"Objectives of front {i}:\", Y_optimal_front)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "# Define the optimization problem\n",
    "creator.create(\"FitnessMulti\", base.Fitness, weights=(-1.0,) * 7)  # Minimize all seven objectives\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMulti)\n",
    "\n",
    "# Define the bounds for your 28 variables (features)\n",
    "BOUND_LOW = [0.0] * 28  # Replace with your lower bounds\n",
    "BOUND_UP = [1.0] * 28   # Replace with your upper bounds\n",
    "\n",
    "# Create the DEAP toolbox\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "def random_float(minval, maxval):\n",
    "    return random.uniform(minval, maxval)\n",
    "\n",
    "# Register attribute, individual initialization, and population creation\n",
    "toolbox.register(\"attr_float\", random_float, BOUND_LOW, BOUND_UP)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_float, n=28)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate(individual):\n",
    "    # Extract the features from the individual\n",
    "    features = np.array(individual)\n",
    "    \n",
    "    # Train your neural network and evaluate it\n",
    "    model = create_and_train_neural_network(features)  # Create and train your neural network\n",
    "    objectives = evaluate_neural_network(model)  # Evaluate the neural network\n",
    "    \n",
    "    return objectives  # Objectives are already in the correct format\n",
    "\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "\n",
    "# Configure NSGA-II parameters\n",
    "MU = 100  # Number of individuals in each generation\n",
    "LAMBDA = 120  # Number of offspring to produce in each generation\n",
    "CXPB = 0.7  # Probability of mating two individuals\n",
    "MUTPB = 0.2  # Probability of mutating an individual\n",
    "NGEN = 1000  # Number of generations\n",
    "\n",
    "# Create the initial population\n",
    "pop = toolbox.population(n=MU)\n",
    "\n",
    "# Run the NSGA-II algorithm\n",
    "algorithms.eaMuPlusLambda(pop, toolbox, MU, LAMBDA, CXPB, MUTPB, NGEN)\n",
    "\n",
    "# Extract the non-dominated fronts\n",
    "fronts = tools.sortNondominated(pop, len(pop), first_front_only=False)\n",
    "\n",
    "# Extract solutions from each front (including all fronts)\n",
    "optimal_solutions = [front[:] for front in fronts]\n",
    "\n",
    "# Extract corresponding variables (X) and objectives (Y) for each front\n",
    "X_optimal_solutions = [np.array([ind for ind in front]) for front in optimal_solutions]\n",
    "Y_optimal_solutions = [np.array([evaluate(ind) for ind in front]) for front in optimal_solutions]\n",
    "\n",
    "# Print or store the optimal solutions, variables, and objectives for all fronts\n",
    "for i, (X_optimal_front, Y_optimal_front) in enumerate(zip(X_optimal_solutions, Y_optimal_solutions)):\n",
    "    print(f\"Optimal solutions from front {i}:\", X_optimal_front)\n",
    "    print(f\"Objectives of front {i}:\", Y_optimal_front)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "# Define the optimization problem\n",
    "creator.create(\"FitnessMulti\", base.Fitness, weights=(-1.0,) * 7)  # Minimize all seven objectives\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMulti)\n",
    "\n",
    "# Define the bounds for your 28 variables (features)\n",
    "BOUND_LOW = [0.0] * 28  # Replace with your lower bounds\n",
    "BOUND_UP = [1.0] * 28   # Replace with your upper bounds\n",
    "\n",
    "# Create the DEAP toolbox\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "def random_float(minval, maxval):\n",
    "    return random.uniform(minval, maxval)\n",
    "\n",
    "# Register attribute, individual initialization, and population creation\n",
    "toolbox.register(\"attr_float\", random_float, BOUND_LOW, BOUND_UP)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_float, n=28)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate(individual):\n",
    "    # Extract the features from the individual\n",
    "    features = np.array(individual)\n",
    "    \n",
    "    # Train your neural network and evaluate it\n",
    "    model = create_and_train_neural_network(features)  # Create and train your neural network\n",
    "    objectives = evaluate_neural_network(model)  # Evaluate the neural network\n",
    "    \n",
    "    return tuple(objectives)  # Return objectives as a tuple\n",
    "\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "\n",
    "# Configure NSGA-II parameters\n",
    "MU = 100  # Number of individuals in each generation\n",
    "LAMBDA = 120  # Number of offspring to produce in each generation\n",
    "CXPB = 0.7  # Probability of mating two individuals\n",
    "MUTPB = 0.2  # Probability of mutating an individual\n",
    "NGEN = 1000  # Number of generations\n",
    "\n",
    "# Create the initial population\n",
    "pop = toolbox.population(n=MU)\n",
    "\n",
    "# Run the NSGA-II algorithm\n",
    "algorithms.eaMuPlusLambda(pop, toolbox, MU, LAMBDA, CXPB, MUTPB, NGEN)\n",
    "\n",
    "# Extract the non-dominated fronts\n",
    "fronts = tools.sortNondominated(pop, len(pop), first_front_only=False)\n",
    "\n",
    "# Extract solutions from each front (including all fronts)\n",
    "optimal_solutions = [front[:] for front in fronts]\n",
    "\n",
    "# Extract corresponding variables (X) and objectives (Y) for each front\n",
    "X_optimal_solutions = [np.array([ind for ind in front]) for front in optimal_solutions]\n",
    "Y_optimal_solutions = [np.array([evaluate(ind) for ind in front]) for front in optimal_solutions]\n",
    "\n",
    "# Print or store the optimal solutions, variables, and objectives for all fronts\n",
    "for i, (X_optimal_front, Y_optimal_front) in enumerate(zip(X_optimal_solutions, Y_optimal_solutions)):\n",
    "    print(f\"Optimal solutions from front {i}:\", X_optimal_front)\n",
    "    print(f\"Objectives of front {i}:\", Y_optimal_front)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "# Define the optimization problem\n",
    "creator.create(\"FitnessMulti\", base.Fitness, weights=(-1.0,) * 7)  # Minimize all seven objectives\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMulti)\n",
    "\n",
    "# Define the bounds for your 28 variables (features)\n",
    "BOUND_LOW = [-1.0] * 28  # Replace with your lower bounds for each variable\n",
    "BOUND_UP = [1.0] * 28   # Replace with your upper bounds for each variable\n",
    "\n",
    "# Create the DEAP toolbox\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "def random_float(minval, maxval):\n",
    "    return random.uniform(minval, maxval)\n",
    "\n",
    "# Register attribute, individual initialization, and population creation\n",
    "toolbox.register(\"attr_float\", random_float, BOUND_LOW, BOUND_UP)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_float, n=28)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate(individual):\n",
    "    # Extract the features from the individual\n",
    "    features = np.array(individual)\n",
    "    \n",
    "    # Train your neural network and evaluate it\n",
    "    model = create_and_train_neural_network(features)  # Create and train your neural network\n",
    "    objectives = evaluate_neural_network(model)  # Evaluate the neural network\n",
    "    \n",
    "    return tuple(objectives)  # Return objectives as a tuple\n",
    "\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "\n",
    "# Configure NSGA-II parameters\n",
    "MU = 100  # Number of individuals in each generation\n",
    "LAMBDA = 120  # Number of offspring to produce in each generation\n",
    "CXPB = 0.7  # Probability of mating two individuals\n",
    "MUTPB = 0.2  # Probability of mutating an individual\n",
    "NGEN = 1000  # Number of generations\n",
    "\n",
    "# Create the initial population\n",
    "pop = toolbox.population(n=MU)\n",
    "\n",
    "# Run the NSGA-II algorithm\n",
    "algorithms.eaMuPlusLambda(pop, toolbox, MU, LAMBDA, CXPB, MUTPB, NGEN)\n",
    "\n",
    "# Extract the non-dominated fronts\n",
    "fronts = tools.sortNondominated(pop, len(pop), first_front_only=False)\n",
    "\n",
    "# Extract solutions from each front (including all fronts)\n",
    "optimal_solutions = [front[:] for front in fronts]\n",
    "\n",
    "# Extract corresponding variables (X) and objectives (Y) for each front\n",
    "X_optimal_solutions = [np.array([ind for ind in front]) for front in optimal_solutions]\n",
    "Y_optimal_solutions = [np.array([evaluate(ind) for ind in front]) for front in optimal_solutions]\n",
    "\n",
    "# Print or store the optimal solutions, variables, and objectives for all fronts\n",
    "for i, (X_optimal_front, Y_optimal_front) in enumerate(zip(X_optimal_solutions, Y_optimal_solutions)):\n",
    "    print(f\"Optimal solutions from front {i}:\", X_optimal_front)\n",
    "    print(f\"Objectives of front {i}:\", Y_optimal_front)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "# Define the optimization problem\n",
    "creator.create(\"FitnessMulti\", base.Fitness, weights=(-1.0,) * 7)  # Minimize all seven objectives\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMulti)\n",
    "\n",
    "# Define the bounds for your 28 variables (features)\n",
    "BOUND_LOW = [-1.0] * 28  # Replace with your lower bounds for each variable\n",
    "BOUND_UP = [1.0] * 28   # Replace with your upper bounds for each variable\n",
    "\n",
    "# Create the DEAP toolbox\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "def random_float(minval, maxval):\n",
    "    return random.uniform(minval, maxval)\n",
    "\n",
    "# Register attribute, individual initialization, and population creation\n",
    "toolbox.register(\"attr_float\", random_float, BOUND_LOW, BOUND_UP)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_float, n=28)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate(individual):\n",
    "    # Extract the features from the individual\n",
    "    features = np.array(individual)\n",
    "    \n",
    "    # Train your neural network and evaluate it\n",
    "    model = create_and_train_neural_network(features)  # Create and train your neural network\n",
    "    objectives = evaluate_neural_network(model)  # Evaluate the neural network\n",
    "    \n",
    "    return tuple(objectives)  # Return objectives as a tuple\n",
    "\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "\n",
    "# Configure NSGA-II parameters\n",
    "MU = 100  # Number of individuals in each generation\n",
    "LAMBDA = 120  # Number of offspring to produce in each generation\n",
    "CXPB = 0.7  # Probability of mating two individuals\n",
    "MUTPB = 0.2  # Probability of mutating an individual\n",
    "NGEN = 1000  # Number of generations\n",
    "\n",
    "# Create the initial population\n",
    "pop = toolbox.population(n=MU)\n",
    "\n",
    "# Run the NSGA-II algorithm\n",
    "algorithms.eaMuPlusLambda(pop, toolbox, MU, LAMBDA, CXPB, MUTPB, NGEN)\n",
    "\n",
    "# Extract the non-dominated fronts\n",
    "fronts = tools.sortNondominated(pop, len(pop), first_front_only=False)\n",
    "\n",
    "# Extract solutions from each front (including all fronts)\n",
    "optimal_solutions = [front[:] for front in fronts]\n",
    "\n",
    "# Extract corresponding variables (X) and objectives (Y) for each front\n",
    "X_optimal_solutions = [np.array([ind for ind in front]) for front in optimal_solutions]\n",
    "Y_optimal_solutions = [np.array([evaluate(ind) for ind in front]) for front in optimal_solutions]\n",
    "\n",
    "# Print or store the optimal solutions, variables, and objectives for all fronts\n",
    "for i, (X_optimal_front, Y_optimal_front) in enumerate(zip(X_optimal_solutions, Y_optimal_solutions)):\n",
    "    print(f\"Optimal solutions from front {i}:\", X_optimal_front)\n",
    "    print(f\"Objectives of front {i}:\", Y_optimal_front)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HHerRZt_v9ab",
    "outputId": "91d910fb-7e71-40bc-b3f4-a2699b037727"
   },
   "outputs": [],
   "source": [
    "\n",
    "row,col= y_pred_original_scale.shape\n",
    "print(row, col)\n",
    "plt.figure(figsize=(15,15))\n",
    "ind =int(np.sqrt(col))+1\n",
    "for i in range(0,col):\n",
    "    plt.subplot(ind,ind,i+1)\n",
    "    plt.hexbin(y_val_original_scale [:,i], y_pred_original_scale [:,i], cmap='Reds')\n",
    "    plt.ylabel('predicted outputs')\n",
    "    plt.xlabel('actual outputs')\n",
    "    plt.tight_layout()\n",
    "plt.show()\n",
    "residuals= (y_pred_original_scale - y_val_original_scale)\n",
    "rsq = np.sum(residuals**2, axis =0)/(np.sum(y_val**2, axis=0))\n",
    "print(rsq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'original_dataset' is your original DataFrame with many columns\n",
    "# and 'columns_to_keep' is a list of 40 column names you want to select\n",
    "\n",
    "# Example list of 40 column names you want to keep\n",
    "features1 = ['setMFX2I01', 'setMFX1I03', 'setMFX0I01', 'setMFA0I03', 'setMFD0I04',\n",
    "       'setMFX1dsch2', 'setMFX2dsch2', 'setMFX3dsch2', 'setMQS0L01',\n",
    "       'setMQJ0L01', 'setMQS0L01A', 'setMQJ0L02', 'setMQS0L02', 'setMQJ0L02A',\n",
    "       'setMQS0L02B', 'setMQJ0L03A', 'setMQS0L03', 'setMQJ0L03', 'setMQS0L04',\n",
    "       'setMQJ0L04', 'PREBUNCHERphase', 'BUNCHERphase', 'booster07phase',\n",
    "       'booster08phase', 'PREBUNCHERset', 'buncherset', 'booster07set',\n",
    "       'booster08set', 'uf:nemixrms', 'uf:nemiyrms', 'uf:stdt', 'uf:stdx','uf:stdy', 'uf:stdEk_eV']\n",
    "\n",
    "# Create a new DataFrame with only the 40 selected columns\n",
    "data1 = df2[features1]\n",
    "\n",
    "# Print the first few rows of the new dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = data1.iloc[:, :28].values  # Extract the first 28 columns as input data (variables)\n",
    "y_new = data1.iloc[:, 28:].values  # Extract the last 7 columns as output data (objectives)\n",
    "y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TnoHuOBKIoTf",
    "outputId": "6c6764dc-fc60-4d49-d290-29fcbf3d3e13"
   },
   "outputs": [],
   "source": [
    "y_pred_original_scale.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8_oJXF1CHcmw",
    "outputId": "5dc4171b-4f26-4592-a74d-e63c60d2af31"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WWwfCt6GLQag"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "id": "Ge3QI7lFJY0-",
    "outputId": "7b0046fe-0203-41cb-ac43-a3ef235c79dc"
   },
   "outputs": [],
   "source": [
    "#column = my_array[:, column_index]\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(y_val_original_scale[:,0]*1e6, y_pred_original_scale[:,0]*1e6, label='norm xemittance')\n",
    "plt.ylabel('predicted outputs')\n",
    "plt.xlabel('actual outputs')\n",
    "plt.legend()\n",
    " #plt.tight_layout()\n",
    "plt.show()\n",
    "#residuals= (y_pred -y_val)\n",
    "#rsq = np.sum(residuals**2, axis =0)/(np.sum(y_val**2, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "o-F4cbUnFnD8",
    "outputId": "ffc14760-aaf7-476a-ca9f-9bacad67c50c"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "ind =int(np.sqrt(col))+1\n",
    "for i in range(0,col):\n",
    " plt.subplot(ind,ind,i+1)\n",
    " plt.scatter(y_val_original_scale[:,i], y_pred_original_scale[:,i])\n",
    " plt.ylabel('predicted outputs')\n",
    " plt.xlabel('actual outputs')\n",
    " #plt.tight_layout()\n",
    "plt.savefig('simvsprediction.png')\n",
    "from google.colab import files\n",
    "files.download( \"simvsprediction.png\" )\n",
    "plt.show()\n",
    "residuals= (y_pred -y_val)\n",
    "rsq = np.sum(residuals**2, axis =0)/(np.sum(y_val**2, axis=0))\n",
    "print(rsq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "id": "QBTL1LNcxZ86",
    "outputId": "aace397d-e6a4-40e3-a439-d73b59f3f4a2"
   },
   "outputs": [],
   "source": [
    "#column = my_array[:, column_index]\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(y_val_original_scale[:,1]*1e6, y_pred_original_scale[:,1]*1e6, label='norm emittance')\n",
    "plt.ylabel('predicted outputs')\n",
    "plt.xlabel('actual outputs')\n",
    "plt.legend()\n",
    " #plt.tight_layout()\n",
    "plt.show()\n",
    "#residuals= (y_pred -y_val)\n",
    "#rsq = np.sum(residuals**2, axis =0)/(np.sum(y_val**2, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "id": "RPTJN0v6yFlN",
    "outputId": "343c70f5-d6ef-4268-96a1-db15f542cddf"
   },
   "outputs": [],
   "source": [
    "#column = my_array[:, column_index]\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(y_val_original_scale[:,3]*100, y_pred_original_scale[:,3]*100, label='norm emittance')\n",
    "plt.ylabel('predicted outputs')\n",
    "plt.xlabel('actual outputs')\n",
    "plt.legend()\n",
    " #plt.tight_layout()\n",
    "plt.show()\n",
    "#residuals= (y_pred -y_val)\n",
    "#rsq = np.sum(residuals**2, axis =0)/(np.sum(y_val**2, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "id": "73hYCcZC7gQv",
    "outputId": "aaa80af8-1c5e-4386-e6e1-3fdf35a56ef0"
   },
   "outputs": [],
   "source": [
    "#column = my_array[:, column_index]\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(y_val_original_scale[:,2]*1e12, y_pred_original_scale[:,2]*1e12, label='norm emittance')\n",
    "plt.ylabel('predicted outputs')\n",
    "plt.xlabel('actual outputs')\n",
    "plt.legend()\n",
    " #plt.tight_layout()\n",
    "plt.show()\n",
    "#residuals= (y_pred -y_val)\n",
    "#rsq = np.sum(residuals**2, axis =0)/(np.sum(y_val**2, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "id": "XOLRpsLS6DAD",
    "outputId": "7f6dbca8-6703-4789-fa59-9d2f4ea2e2d0"
   },
   "outputs": [],
   "source": [
    "#column = my_array[:, column_index]\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(y_val_original_scale[:,1]*1e6, y_pred_original_scale[:,1]*1e6, label='norm emittance')\n",
    "plt.ylabel('predicted outputs')\n",
    "plt.xlabel('actual outputs')\n",
    "plt.legend()\n",
    " #plt.tight_layout()\n",
    "plt.show()\n",
    "#residuals= (y_pred -y_val)\n",
    "#rsq = np.sum(residuals**2, axis =0)/(np.sum(y_val**2, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fiu2EvV0-MSv",
    "outputId": "23fe3844-d36b-432a-c0a0-91c3a9d2d44c"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from deap import base, creator, tools, algorithms\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Define the evaluation function for your objectives using the given neural network\n",
    "def evaluate_objectives(individual):\n",
    "    # Split the individual's weights into corresponding layer shapes\n",
    "    shapes = [w.shape for w in model.get_weights()]\n",
    "    weights = []\n",
    "    start = 0\n",
    "\n",
    "    for shape in shapes:\n",
    "        size = np.prod(shape)\n",
    "        weight = individual[start:start+size].reshape(shape)\n",
    "        weights.append(weight)\n",
    "        start += size\n",
    "\n",
    "    model.set_weights(weights)\n",
    "\n",
    "    # Calculate the predictions for the test dataset\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    # Calculate the mean squared error for each objective\n",
    "    mse_objectives = np.mean((predictions - y_test)**2, axis=0)\n",
    "\n",
    "    # Return the mean squared error values as a tuple (Objective 1, Objective 2, ..., Objective 7)\n",
    "    return tuple(mse_objectives)\n",
    "\n",
    "# Define the number of objectives and their types (minimize or maximize)\n",
    "num_objectives = 7\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,) * num_objectives)\n",
    "\n",
    "# Define the individual representation and the population type\n",
    "creator.create(\"Individual\", np.ndarray, fitness=creator.FitnessMin)\n",
    "\n",
    "# Create a toolbox for evolution\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "# Register the evaluation function and the individual generator\n",
    "toolbox.register(\"evaluate\", evaluate_objectives)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, lambda: np.random.uniform(-1, 1), n=sum(np.prod(w.shape) for w in model.get_weights()))\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# Genetic operators\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=0.1, indpb=0.1)\n",
    "toolbox.register(\"select\", tools.selNSGA2)\n",
    "\n",
    "def main():\n",
    "    population_size = 100\n",
    "    num_generations = 50\n",
    "\n",
    "    population = toolbox.population(n=population_size)\n",
    "\n",
    "    # Evaluate the initial population\n",
    "    fitnesses = map(toolbox.evaluate, population)\n",
    "    for ind, fit in zip(population, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    # Perform the evolution\n",
    "    for generation in range(num_generations):\n",
    "        offspring = algorithms.varAnd(population, toolbox, cxpb=0.5, mutpb=0.1)\n",
    "        fits = toolbox.map(toolbox.evaluate, offspring)\n",
    "        for ind, fit in zip(offspring, fits):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        population = toolbox.select(population + offspring, k=population_size)\n",
    "\n",
    "    # Extract the Pareto front from the final population\n",
    "    pareto_front = tools.selBest(population, len(population))\n",
    "\n",
    "    return pareto_front\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace X_train, y_train, X_test, and y_test with your actual training and test datasets\n",
    "    pareto_front = main()\n",
    "    print(pareto_front)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have the Pareto front obtained from your main function\n",
    "\n",
    "# Extract the objectives from the Pareto front\n",
    "objectives = [ind.fitness.values for ind in pareto_front]\n",
    "\n",
    "# Separate the objectives into individual lists\n",
    "objective1 = [obj[0] for obj in objectives]\n",
    "objective2 = [obj[1] for obj in objectives]\n",
    "\n",
    "# Create a scatter plot of the Pareto front\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(objective1, objective2, c='b', marker='o', label='Pareto Front')\n",
    "plt.xlabel('Objective 1')\n",
    "plt.ylabel('Objective 2')\n",
    "plt.title('Pareto Front')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have the Pareto front obtained from your main function\n",
    "\n",
    "# Extract the objectives from the Pareto front\n",
    "objectives = [ind.fitness.values for ind in pareto_front]\n",
    "\n",
    "# Separate the objectives into individual lists\n",
    "objective1 = [obj[0] for obj in objectives]\n",
    "objective2 = [obj[2] for obj in objectives]\n",
    "\n",
    "# Create a scatter plot of the Pareto front\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(objective1, objective2, c='b', marker='o', label='Pareto Front')\n",
    "plt.xlabel('Objective 1')\n",
    "plt.ylabel('Objective 2')\n",
    "plt.title('Pareto Front')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have the Pareto front obtained from your main function\n",
    "\n",
    "# Extract the objectives from the Pareto front\n",
    "objectives = [ind.fitness.values for ind in pareto_front]\n",
    "\n",
    "# Separate the objectives into individual lists\n",
    "objective1 = [obj[2] for obj in objectives]\n",
    "objective2 = [obj[5] for obj in objectives]\n",
    "\n",
    "# Create a scatter plot of the Pareto front\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(objective1, objective2, c='b', marker='o', label='Pareto Front')\n",
    "plt.xlabel('Objective 1')\n",
    "plt.ylabel('Objective 2')\n",
    "plt.title('Pareto Front')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have the Pareto front obtained from your main function\n",
    "#pareto_front = main()\n",
    "\n",
    "# Extract the objectives from the Pareto front\n",
    "objectives = [ind.fitness.values for ind in pareto_front]\n",
    "\n",
    "# Normalize the objectives (for example, to [0, 1] range)\n",
    "min_val = np.min(objectives, axis=0)\n",
    "max_val = np.max(objectives, axis=0)\n",
    "normalized_objectives = (objectives - min_val) / (max_val - min_val)\n",
    "\n",
    "# Separate the normalized objectives into individual lists\n",
    "objective1 = [obj[0] for obj in normalized_objectives]\n",
    "objective2 = [obj[1] for obj in normalized_objectives]\n",
    "\n",
    "# Create a scatter plot of the Pareto front\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(objective1, objective2, c='b', marker='o', label='Pareto Front')\n",
    "plt.xlabel('Normalized Objective 1')\n",
    "plt.ylabel('Normalized Objective 2')\n",
    "plt.title('Pareto Front (Normalized)')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto_front_original_scale = scaler.inverse_transform(pareto_front)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 728
    },
    "id": "7bW5Lx3RD3Qm",
    "outputId": "ed96292f-e535-40c9-e541-1478e99731ad"
   },
   "outputs": [],
   "source": [
    "# Extract the objective values from the Pareto front for Objective 1 and Objective 2\n",
    "objectives = np.array([ind.fitness.values for ind in pareto_front])\n",
    "objective1_values = objectives[:, 0]\n",
    "objective2_values = objectives[:, 2]\n",
    "obj1= scaler.inverse_transform(objective1_values)\n",
    "obj2= scaler.inverse_transform(objective2_values)\n",
    "\n",
    "# Plot the Pareto front for Objective 1 and Objective 2\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(objective1_values, objective2_values)\n",
    "plt.xlabel('Objective 1')\n",
    "plt.ylabel('Objective 2')\n",
    "plt.title('Pareto Front (Objective 1 vs Objective 2)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_MTBgmyB80OE",
    "outputId": "12375820-c1e2-4f8a-c3fa-1c4161bfbc1e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "split=80\n",
    "# Step 1: Split data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size = (100 - split) / 100.,random_state = 42)\n",
    "# Step 2: Further split the training set into training and validation sets (75% training, 25% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Define the model architecture\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Add four hidden layers, each with 40 nodes and hyperbolic tangent activation function.\n",
    "model.add(layers.Dense(40, activation='tanh', input_shape=(28,)))\n",
    "model.add(layers.Dense(40, activation='tanh'))\n",
    "model.add(layers.Dense(40, activation='tanh'))\n",
    "model.add(layers.Dense(40, activation='tanh'))\n",
    "\n",
    "# Add the output layer with 7 nodes (one for each objective)\n",
    "model.add(layers.Dense(7))\n",
    "\n",
    "# Compile the model with Adam optimizer and appropriate learning rate\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "#split=80\n",
    "# Step 1: Split data into training and testing sets (80% training, 20% testing)\n",
    "#X_train, X_test, X_val, y_train, y_test, y_val = train_test_split(X_scaled, y_scaled, test_size = (100 - split) / 100., val_size=(100 - split) / 100., random_state = 42)\n",
    "# Step 2: Further split the training set into training and validation sets (75% training, 25% validation)\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "# Split the data into training, validation, and test sets\n",
    "# Assuming you have a function or method to split the data into train, val, and test sets\n",
    "#X_train, X_val, X_test, y_train, y_val, y_test = split_data(X_scaled, y_scaled)\n",
    "\n",
    "# Train the model for 10,000 epochs with a batch size of 500 points\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=100, validation_data=(X_val, y_val), shuffle = 'true',verbose = 'false')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss1 = model.evaluate(X_train, y_train)\n",
    "loss2 = model.evaluate(X_test, y_test)\n",
    "loss3 = model.evaluate(X_val, y_val)\n",
    "print(\"Test loss:\", loss1, loss2, loss3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "id": "kfiggrE3jliq",
    "outputId": "4759cf6a-c715-4fdd-9485-5ae566fea71a"
   },
   "outputs": [],
   "source": [
    "# Create a MirroredStrategy for data parallelism\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# Define and compile your model within the strategy's scope\n",
    "with strategy.scope():\n",
    "    # Define the model architecture\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # Add four hidden layers, each with 20 nodes and hyperbolic tangent activation function.\n",
    "    model.add(layers.Dense(20, activation='tanh', input_shape=(28,)))\n",
    "    model.add(layers.Dense(20, activation='tanh'))\n",
    "    model.add(layers.Dense(20, activation='tanh'))\n",
    "    model.add(layers.Dense(20, activation='tanh'))\n",
    "\n",
    "    # Add the output layer with 12 nodes (one for each objective)\n",
    "    model.add(layers.Dense(7))\n",
    "\n",
    "    # Compile the model with Adam optimizer and appropriate learning rate\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Scale the input data to fit within the range of [-1, 1]\n",
    "def scale_data(df1):\n",
    "    min_val = np.min(data)\n",
    "    max_val = np.max(data)\n",
    "    scaled_data = -1 + 2 * (data - min_val) / (max_val - min_val)\n",
    "    return scaled_data\n",
    "\n",
    "X_scaled = scale_data(X)\n",
    "y_scaled = scale_data(y)\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "# Assuming you have a function or method to split the data into train, val, and test sets\n",
    "#X_train, X_val, X_test, Y_train, Y_val, Y_test = split_data(X_scaled, Y_scaled)\n",
    "\n",
    "# Train the model for 10,000 epochs with a batch size of 500 points\n",
    "model.fit(X_train, _train, epochs=10, batch_size=50, validation_data=(X_val, Y_val))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss = model.evaluate(X_test, Y_test)\n",
    "print(\"Test loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IWYjHA5Y3wLF",
    "outputId": "c1b43da0-2231-4d72-8e32-529ddf1abb8f"
   },
   "outputs": [],
   "source": [
    "!pip install keras-tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2G2xj0-3X1q"
   },
   "source": [
    "**Hyperparameter Tuning: in keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z50MJWwy2-4g"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import keras_tuner\n",
    "from keras_tuner.tuners import RandomSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvjmDyLW3Whs"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_sT1rHS29tr"
   },
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=((8000,28),)))  # Replace 'input_shape' with the appropriate value\n",
    "    for i in range(hp.Int('num_layers', 1, 4)):  # Number of layers (1 to 4)\n",
    "        model.add(layers.Dense(units=hp.Int('units_' + str(i), 32, 256, 32), activation='tanh'))  # Units per layer (32 to 256 in steps of 32)\n",
    "    model.add(layers.Dense(units=7, activation='softmax'))  # Output layer, replace 'num_classes' with the number of classes in your problem\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "ns7XRYdIj6i2",
    "outputId": "12fcc2a9-cebe-4c37-b09d-4a73481dca9e"
   },
   "outputs": [],
   "source": [
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',  # The metric to optimize (replace with 'val_loss' for loss minimization)\n",
    "    max_trials=10,  # Total number of trials\n",
    "    executions_per_trial=2,  # Number of times to train each model (reduces the impact of random initialization)\n",
    "    directory='my_dir',  # Directory to store the search results\n",
    "    project_name='my_project'  # Name of the search project\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-6O30OHn7e8c",
    "outputId": "7972950e-6564-49c0-be8f-5c40235b6011"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Create a function that returns the Keras model\n",
    "def create_model(optimizer='adam', activation='relu', units=7):\n",
    "    model = Sequential()\n",
    "    # Add layers to your model\n",
    "    # ...\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Wrap the Keras model with KerasClassifier for compatibility with GridSearchCV\n",
    "keras_model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "# Define hyperparameters to tune\n",
    "param_grid = {\n",
    "    'optimizer': ['adam', 'sgd'],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'units': [32, 64, 128]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=keras_model, param_grid=param_grid, cv=3)\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and corresponding accuracy\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "aOEgBbkD8_gE",
    "outputId": "00dc1271-52d0-421a-f40f-cedee8f754ad"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "#from scikeras.wrappers import TensorFlowModelWrapper\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Prepare your multi-input, multi-output (MIMO) Keras model\n",
    "def create_model(optimizer='adam', activation='relu', units=32):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units, activation=activation, input_shape=(28,)))\n",
    "    # Add more layers as needed for your model architecture\n",
    "    # ...\n",
    "    model.add(Dense(7))  # Output layer with 7 nodes for 7 output objectives\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Step 2: Define your evaluation function to return a list of loss values for all output objectives\n",
    "def evaluate_model(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    losses = np.mean((y_pred - y)**2, axis=0)  # Compute mean squared error for each output objective\n",
    "    return losses.tolist()\n",
    "\n",
    "# Step 3: Prepare your data and use RandomizedSearchCV for hyperparameter tuning\n",
    "# Assuming you have your input data X and output data y\n",
    "\n",
    "# Split your data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Wrap the Keras model with KerasRegressor for compatibility with RandomizedSearchCV\n",
    "# Wrap the Keras model with TensorFlowModelWrapper for compatibility with RandomizedSearchCV\n",
    "keras_model = KerasRegressor(build_fn=create_model, epochs=10, batch_size=32, verbose=0)(build_fn=create_model, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "# Define hyperparameter distributions to sample from\n",
    "param_dist = {\n",
    "    'optimizer': ['adam', 'sgd'],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'units': [20, 40]\n",
    "}\n",
    "\n",
    "# Perform random search\n",
    "random_search = RandomizedSearchCV(estimator=keras_model, param_distributions=param_dist, cv=3, n_iter=5)\n",
    "random_result = random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and corresponding loss values\n",
    "print(\"Best: %f using %s\" % (random_result.best_score_, random_result.best_params_))\n",
    "print(\"Validation Losses for Best Model:\", evaluate_model(random_result.best_estimator_, X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!brew install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Create a neural network model\n",
    "model = tf.keras.Sequential([\n",
    "    Dense(40, activation='tanh', input_shape=(28,)),\n",
    "    Dense(40, activation='tanh'),\n",
    "    Dense(40, activation='tanh'),\n",
    "    Dense(40, activation='tanh'),\n",
    "    Dense(7)  # Output layer with 7 nodes for objectives\n",
    "])\n",
    "\n",
    "# Visualize the neural network architecture\n",
    "tf.keras.utils.plot_model(model, to_file='neural_network.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "# Show the plot\n",
    "img = plt.imread('neural_network.png')\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Create a neural network model\n",
    "model = tf.keras.Sequential([\n",
    "    Dense(40, activation='tanh', input_shape=(28,)),\n",
    "    Dense(40, activation='tanh'),\n",
    "    Dense(40, activation='tanh'),\n",
    "    Dense(40, activation='tanh'),\n",
    "    Dense(7)  # Output layer with 7 nodes for objectives\n",
    "])\n",
    "\n",
    "# Visualize the neural network architecture\n",
    "tf.keras.utils.plot_model(model, to_file='neural_network.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "# Show the plot\n",
    "img = plt.imread('neural_network.png')\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Create a neural network model\n",
    "model = tf.keras.Sequential([\n",
    "    Dense(40, activation='tanh', input_shape=(28,)),  # Layer 1\n",
    "    Dense(40, activation='tanh'),                    # Layer 2\n",
    "    Dense(40, activation='tanh'),                    # Layer 3\n",
    "    Dense(40, activation='tanh'),                    # Layer 4\n",
    "    Dense(7)                                         # Layer 5 (Output layer with 7 nodes for objectives)\n",
    "])\n",
    "\n",
    "# Visualize the neural network architecture with custom appearance\n",
    "plot_model = tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    to_file='neural_network.png',\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    dpi=96  # Set the DPI for better quality\n",
    ")\n",
    "\n",
    "# Show the plot with custom fonts and colors\n",
    "img = plt.imread('neural_network.png')\n",
    "plt.figure(figsize=(10, 10))  # Increase the figure size\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "\n",
    "# Customize fonts and colors (you can adjust these as needed)\n",
    "plt.rcParams.update({\n",
    "    'text.color': 'white',  # Font color\n",
    "    'font.size': 14,        # Font size\n",
    "    'axes.labelcolor': 'white',\n",
    "    'axes.edgecolor': 'white',\n",
    "    'xtick.color': 'white',\n",
    "    'ytick.color': 'white',  # Figure background color\n",
    "})\n",
    "#plt.tight_layout()\n",
    "plt.savefig('neuralnetwork.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Create a neural network model\n",
    "model = tf.keras.Sequential([\n",
    "    Dense(40, activation='tanh', input_shape=(28,)),  # Layer 1\n",
    "    Dense(40, activation='tanh'),                    # Layer 2\n",
    "    Dense(40, activation='tanh'),                    # Layer 3\n",
    "    Dense(40, activation='tanh'),                    # Layer 4\n",
    "    Dense(7)                                         # Layer 5 (Output layer with 7 nodes for objectives)\n",
    "])\n",
    "\n",
    "# Visualize the neural network architecture with custom appearance\n",
    "plot_model = tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    to_file='neural_network.png',\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    dpi=96  # Set the DPI for better quality\n",
    ")\n",
    "\n",
    "# Show the plot with custom fonts and colors\n",
    "img = plt.imread('neural_network.png')\n",
    "\n",
    "# Get the dimensions of the image\n",
    "height, width, _ = img.shape\n",
    "\n",
    "# Set the aspect ratio to match the image dimensions\n",
    "plt.figure(figsize=(width / 100, height / 100))  # Adjust figure size based on image dimensions\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "\n",
    "# Customize fonts and colors (you can adjust these as needed)\n",
    "plt.rcParams.update({\n",
    "    'text.color': 'white',  # Font color\n",
    "    'font.size': 14,        # Font size\n",
    "    'axes.labelcolor': 'white',\n",
    "    'axes.edgecolor': 'white',\n",
    "    'xtick.color': 'white',\n",
    "    'ytick.color': 'white',  # Figure background color\n",
    "})\n",
    "\n",
    "# Save the plot with tight layout and without black space outside\n",
    "plt.tight_layout()\n",
    "plt.savefig('neuralnetwork.png', bbox_inches='tight', pad_inches=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Create a neural network model\n",
    "model = tf.keras.Sequential([\n",
    "    Dense(40, activation='tanh', input_shape=(28,)),  # Layer 1\n",
    "    Dense(40, activation='tanh'),                    # Layer 2\n",
    "    Dense(40, activation='tanh'),                    # Layer 3\n",
    "    Dense(40, activation='tanh'),                    # Layer 4\n",
    "    Dense(7)                                         # Layer 5 (Output layer with 7 nodes for objectives)\n",
    "])\n",
    "\n",
    "# Visualize the neural network architecture with custom appearance\n",
    "plot_model = tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    to_file='neural_network.png',\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    dpi=96  # Set the DPI for better quality\n",
    ")\n",
    "\n",
    "# Show the plot with custom fonts and colors\n",
    "img = plt.imread('neural_network.png')\n",
    "\n",
    "# Get the dimensions of the image\n",
    "height, width, _ = img.shape\n",
    "\n",
    "# Set the aspect ratio to match the image dimensions\n",
    "plt.figure(figsize=(width / 100, height / 100))  # Adjust figure size based on image dimensions\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "\n",
    "# Customize fonts and colors (you can adjust these as needed)\n",
    "plt.rcParams.update({\n",
    "    'text.color': 'white',  # Font color\n",
    "    'font.size': 14,        # Font size\n",
    "    'axes.labelcolor': 'white',\n",
    "    'axes.edgecolor': 'white',\n",
    "    'xtick.color': 'white',\n",
    "    'ytick.color': 'white',  # Figure background color\n",
    "})\n",
    "\n",
    "# Save the plot with tight layout and without black space outside\n",
    "plt.tight_layout()\n",
    "plt.savefig('neuralnetwork.png', bbox_inches='tight', pad_inches=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
